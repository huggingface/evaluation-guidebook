# 리워드 모델에 대해서는?

## 리워드 모델이란 무엇인가?

리워드 모델은 주어진 프롬프트/완성 쌍에 대한 인간 주석으로부터 점수를 예측하는 방법을 학습합니다. 궁극적인 목표는 인간의 선호도에 부합하는 예측을 수행하는 것입니다.
학습이 완료되면, 이러한 모델은 인간의 판단을 대신하는 리워드 함수 역할을 함으로써 다른 모델을 개선하는 데 사용될 수 있습니다.

### 쌍별 점수

가장 일반적인 리워드 모델 유형은 Bradley-Terry 모델로, 다음과 같이 단일 점수를 출력합니다:

$$p(\text{완성 b가 완성 a보다 낫다}) = \text{sigmoid}(\text{점수}_b - \text{점수}_a)$$

이 모델은 완성된 결과의 쌍별 비교만을 사용하여 훈련되며, 이는 점수보다 수집하기 쉽지만 하나의 프롬프트에 대한 여러 완성 결과만 비교할 수 있고 프롬프트 간의 완성 결과는 비교할 수 없습니다.

다른 모델들은 이 접근 방식을 확장하여 한 완성이 다른 것보다 더 나은지에 대한 더 미묘한 확률을 예측합니다 ([예시](https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B)).

이를 통해 (이론적으로) 완성 간의 미묘한 차이를 판단할 수 있지만, 동일한 테스트 세트에 대해 프롬프트 간에 많은 다양한 점수를 쉽게 저장하고 비교할 수 없다는 단점이 있습니다. 또한, 너무 긴 완성 결과를 비교할 때 컨텍스트 길이와 메모리 제한이 문제가 될 수 있습니다.

### 절대 점수

[SteerLM](https://arxiv.org/abs/2311.09528)과 같은 일부 리워드 모델은 절대 점수를 출력하며, 이는 쌍별 비교 없이 완성 결과를 직접 평가하는 데 사용할 수 있습니다. 이러한 모델은 평가에 더 쉽게 사용할 수 있지만, 절대 점수가 인간 선호도에서 쌍별 점수보다 덜 안정적인 경향이 있기 때문에 데이터 수집이 더 어렵습니다.

최근에는 [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)와 [ArmoRM](https://arxiv.org/abs/2406.12845)과 같이 절대 점수와 상대 점수를 모두 출력하는 모델이 제안되었습니다.


## 평가를 위해 리워드 모델을 어떻게 사용하나요?

프롬프트 데이터셋이 주어지면, 언어 모델에서 완성 결과를 생성하고 리워드 모델에 이를 평가하도록 요청할 수 있습니다.

절대 점수를 제공하는 모델의 경우, 결과 점수를 평균하여 합리적인 요약 점수를 얻을 수 있습니다.

그러나, 더 일반적인 상대 점수의 경우, 평균 리워드는 이상치(매우 좋거나 매우 나쁜 몇몇 완성 결과)에 의해 편향될 수 있으며, 서로 다른 프롬프트는 본질적으로 다른 리워드 척도를 가질 수 있습니다(일부 프롬프트는 다른 것보다 훨씬 어렵거나 쉬울 수 있음).

대신, 다음을 사용할 수 있습니다:
- 승률: 참조 완성 결과 세트를 가져와 모델의 완성 결과 중 참조 완성 결과보다 높게 순위가 매겨진 비율을 계산합니다. 이는 약간 더 세분화된 방법입니다.
- 승리 확률: 완성 결과가 참조 완성 결과보다 더 나을 확률의 평균으로, 더 세밀하고 부드럽게 변화하는 신호를 제공할 수 있습니다.

## 리워드 모델의 장단점

리워드 모델은 일반적으로:
- **매우 빠름**: 점수를 얻는 것은 상대적으로 작은 모델의 순방향 패스를 한 번 실행하는 것만큼 간단합니다(평가 LLM과 달리 긴 텍스트가 아닌 점수만 얻기 때문).
- **결정론적**: 동일한 순방향 패스를 통해 동일한 점수가 재현됩니다.
- **위치 편향을 겪을 가능성이 낮음**: 대부분의 모델이 하나의 완성만 받기 때문에 순서에 영향을 받을 수 없습니다. 쌍별 모델의 경우, 훈련 데이터가 첫 번째와 두 번째 답변 모두 최선으로 균형을 이루었다면 위치 편향도 최소화됩니다.
- **프롬프트 엔지니어링이 필요 없음**: 모델이 훈련된 선호도 데이터에 따라 하나 또는 두 개의 완성 결과에서 점수를 출력하기 때문입니다.

반면에:
- **특정 미세 조정이 필요**: 이는 상대적으로 비용이 많이 드는 단계일 수 있으며, 기본 모델에서 많은 기능을 상속받지만 훈련 분포 밖의 작업에서는 여전히 성능이 좋지 않을 수 있습니다.
- **강화 학습과 평가 모두에 사용할 때 효율성 저하** (또는 리워드 모델의 훈련 데이터와 유사한 데이터셋에 직접 정렬 알고리즘을 사용할 때), 언어 모델이 리워드 모델의 선호도에 과적합될 수 있습니다.

## 평가를 위한 리워드 모델 사용 팁과 요령

- 고성능 모델을 찾는 좋은 장소는 [RewardBench 리더보드](https://huggingface.co/spaces/allenai/reward-bench)입니다.
- [Nemotron](https://arxiv.org/abs/2406.11704) 논문에서 리워드 모델이 어떻게 사용되었는지 살펴볼 수 있습니다.
- 단일 프롬프트와 완성을 평가하는 리워드 모델의 경우, 많은 참조 모델의 점수를 캐싱하고 새 모델의 성능을 쉽게 확인할 수 있습니다.
- [이](https://arxiv.org/abs/2410.11677v1) 최근 논문에서와 같이 훈련에 걸쳐 승률이나 확률을 추적하면 모델 성능 저하를 감지하고 최적의 체크포인트를 선택할 수 있습니다.