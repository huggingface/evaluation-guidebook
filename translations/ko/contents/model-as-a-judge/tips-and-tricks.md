# 팁과 요령

## LLM 평가자의 잘 알려진 편향 완화하기: 

- **내부 일관성 부족**: 평가자에게 여러 번 프롬프트를 제시하면 다른 판단을 내릴 수 있습니다(온도가 0이 아닌 경우)
	- 이는 평가자에게 자기 일관성 프롬프팅을 수행하여, 여러 번 프롬프트를 제시하고 다수의 출력을 유지함으로써 완화할 수 있습니다
- **자기 선호**: 답변을 평가할 때 [자신의 출력을 선호하는 경향](https://arxiv.org/abs/2404.13076)이 있습니다
	- 이는 배심원단을 사용하여 완화할 수 있습니다
- **입력 변형에 대한 무감각**: 모델은 [변형된 입력](https://arxiv.org/abs/2406.13439)을 식별하는 데 취약하며, 부수적으로 [일관된 점수 범위를 제공하는 데 취약합니다](https://twitter.com/aparnadhinak/status/1748368364395721128)([여기](https://github.com/LeonEricsson/llmjudge/blob/main/README.md)에서 확장된 실험 참조). 예를 들어, 일관된 척도로 노이즈가 추가된 텍스트의 품질을 평가하도록 요청받으면, 예측된 등급이 이 척도를 반영하지 않습니다.
	- 다음과 같은 방법으로 이를 완화할 수 있습니다:
		- [점수를 제공하기 전에](https://twitter.com/seungonekim/status/1749289437165769177) 모델에게 추론을 설명하도록 요청
		- 프롬프트에 일관된 채점 척도 제공
- **위치 편향**: [특정 답변 위치를 선호하는 경향](https://arxiv.org/abs/2306.05685)이 있습니다. 예를 들어, 쌍별 비교에서 Claude와 GPT3.5는 꽤 체계적으로 첫 번째 선택이나 두 번째 선택을 선호하는 경향이 있습니다
	- 다음과 같은 방법으로 이를 완화할 수 있습니다:
		- 무작위로 답변 위치 변경
		- 모든 가능한 선택의 로그 확률을 계산하여 정규화된 답변 얻기
- **장황함 편향**(또는 길이 편향): 더 장황한 답변을 선호하는 경향이 있습니다
	- [답변 길이 차이를 고려하여](https://arxiv.org/abs/2404.04475) 이를 완화할 수 있습니다
- **[인간 답변과의](https://arxiv.org/abs/2308.15812) 일관성에 대한 논쟁:**
	- 그러나 [비전문가 인간이 모든 평가에 좋은 기준점인지는 논쟁의 여지](https://arxiv.org/abs/2202.06935)가 있습니다. 일부 특정 도메인(의학, 법률, 수학 등)에서는 비전문가 인간 평가자에 의존하는 것이 LLM을 직접 사용하는 것만큼 나쁜 기준점이 될 수 있습니다.
- **형식 편향**: 프롬프트 형식이 훈련된 것과 [너무 다르면](https://arxiv.org/abs/2310.17631) 정확하게 평가하지 못하는 경향이 있습니다. 예를 들어, 추가된 참조 답변이 있는 쌍별 비교를 수행하도록 훈련된 모델은 해당 답변이 제공되지 않으면 실패하며, 반대의 경우에도 실패가 발생합니다.
	- 훈련 프롬프트 형식(모델이 지시 튜닝된 경우)에 주의를 기울이고 이를 따르도록 하여 이를 완화할 수 있습니다.

## LLM 평가자에게 적합한 작업 선택하기

LLM 평가자는:
- 일반적으로 **환각을 식별하는 데 취약**하며, 특히 부분 환각(실제 진실과 비슷해 보이지만 실제로는 약간 다름)을 식별하는 데 취약합니다([이것](https://arxiv.org/abs/2305.11747)과 [이것](https://arxiv.org/abs/2303.08896) 참조)
- [요약](https://arxiv.org/abs/2304.02554)([여기도](https://arxiv.org/abs/2303.16634)), [충실도](https://arxiv.org/abs/2307.16877)에 관해 인간 평가자와의 상관관계가 낮거나 그저 그런 수준이며, [다양한 작업 범위](https://arxiv.org/abs/2406.18403)에서 인간 판단과 일관되게 상관관계가 있지 않습니다