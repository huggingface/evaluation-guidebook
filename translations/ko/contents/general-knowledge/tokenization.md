# 토큰화(Tokenization)

## 왜, 그리고 어떻게 텍스트를 토큰화할까요?
대규모 언어 모델은 본질적으로 거대한 수학적 함수이기 때문에 텍스트가 아닌 숫자를 입력으로 받습니다.

텍스트를 숫자로 변환하려면, 먼저 문장을 작은 조각으로 나누는 방법을 결정한 다음, 각 조각을 숫자에 매핑해야 합니다. 이 과정을 *토큰화(tokenization)*라고 합니다.

영어의 경우, 과거에는 텍스트의 각 문자를 알파벳 내 인덱스에 매핑하는 방식(`a` -> 1, `b` -> 2, 등)을 사용했는데, 이를 *문자 기반 토큰화(character based tokenization)*라고 합니다(문자 단위로 분할). 반대편에서는 각 단어를 사전 내 인덱스에 매핑하는 방식(`a` -> 1, `aardvark` -> 2, `ab` -> 3, 등)을 시도했는데, 이를 *단어 기반 토큰화(word based tokenization)*라고 합니다(공백으로 분할, 언어에 공백이 있는 경우 - 없다면 좀 더 복잡해집니다).

이 두 방법 모두 큰 한계를 가지고 있습니다: 입력 텍스트에서 정보를 제거합니다. 단어 형태에서 볼 수 있는 의미적 연결(예: `dis similar`, `similar`, `similar ity`, `similar ly`)을 지워버리는데, 이는 모델이 관련 단어들을 연결할 수 있도록 유지하고 싶은 정보입니다.
(게다가, 입력에 완전히 새로운 단어가 있으면 어떻게 될까요? 그 단어는 숫자를 할당받지 못하고, 모델은 이를 처리할 수 없게 됩니다 😔)

따라서 일부 연구자들은 단어를 하위 단어(sub-words)로 나누고, 이 하위 단어들에 인덱스를 할당하는 아이디어를 제안했습니다(`dis`, `similar`, `ity`, `ly`)!

처음에는 형태통사론적 규칙(morpho-syntactic rules, 단어 생성의 문법과 같은 것)을 사용했습니다. 현재는 대부분 바이트 페어 인코딩(BPE, Byte Pair Encoding)을 사용하는데, 이는 참조 텍스트에서의 빈도에 따라 하위 단어를 자동으로 생성하는 스마트한 통계적 방법입니다.

요약하자면: 토큰화는 텍스트의 작은 단위(한 글자부터 여러 글자, 심지어 단어 수준까지)를 숫자(인덱스와 유사)에 매핑하는 방법입니다. 텍스트를 처리할 때, 입력 텍스트(추론 시 *프롬프트(prompt)*라고 함)는 토크나이저에 의해 이러한 *토큰(tokens)*으로 분할됩니다. 모델이나 토크나이저가 처리할 수 있는 전체 토큰 범위를 *어휘(vocabulary)*라고 합니다.

#### 더 알아보기: 토큰화 이해하기
처음 두 링크 중 하나를 자세히 읽어보시기 바랍니다.
- ⭐ [🤗 NLP 코스에서 다양한 토큰화 방법에 대한 설명](https://huggingface.co/learn/nlp-course/en/chapter2/4)
- ⭐ [🤗 문서에서 토큰화에 대한 개념 가이드](https://huggingface.co/docs/transformers/en/tokenizer_summary)
- [주라프스키(Jurafsky)의 토큰화 강의(및 기타 내용)](https://web.stanford.edu/~jurafsky/slp3/2.pdf) - 접근 방식이 더 학술적입니다. 2.5와 2.6로 건너뛰세요(나머지도 흥미롭지만 범위가 너무 넓습니다)

#### 더 알아보기: 바이트 페어 인코딩(BPE)
- ⭐ [🤗 NLP 코스에서 BPE에 대한 설명](https://huggingface.co/learn/nlp-course/en/chapter6/5)
- [BPE를 NLP에 도입한 논문](https://aclanthology.org/P16-1162/)


## 토큰화의 여러 문제점들
### 적절한 어휘 크기 선택하기
어휘의 크기는 모델이 학습해야 할 개별 토큰(예: 하위 단어)의 수를 나타냅니다.

**너무 큰** 어휘는 전체 토큰으로 매우 희귀한 단어를 포함할 수 있으며(예: `aardvark`), 이는 두 가지 문제를 일으킬 수 있습니다.

만약 그런 희귀한 단어가 학습 데이터에 거의 나타나지 않는다면, 다른 개념들과 연결하기 어려울 수 있고, 모델은 그것이 무엇인지 추론할 수 없을 것입니다.

반면, 드물게 그리고 특정 맥락에서만 나타난다면, 매우 특정한 단어들과 연결될 수 있습니다. 예를 들어, 포럼 데이터로 학습할 때 토크나이저가 사용자 이름을 하나의 토큰으로 매핑했다면, 모델은 이 토큰을 해당 사용자의 특정 콘텐츠와 연관시킬 수 있습니다.

**너무 작은** 어휘는 다른 두 가지 문제를 가집니다: 표현 능력 저하와 추론 비용 증가입니다.

앞서 살펴본 예시로 돌아가서, `similar`에서 파생된 단어들을 토큰화해 봅시다. 유사 BPE 접근법(대규모 어휘)을 사용하여 `similarly`를 토큰화하면 두 개의 토큰(`similar`, `ly`)으로 분할됩니다. 반면에 문자 수준 토큰화(따라서 알파벳 크기의 매우 작은 어휘)를 사용했다면, 같은 단어가 9개의 토큰(`s`, `i`, `m`, `i`, `l`, `a`, `r`, `l`, `y`)으로 분할됩니다.

첫 번째 방법은 `similarly`를 개별적인 의미를 가진 토큰으로 분할하지만, 두 번째 방법은 그렇지 않습니다. 너무 작은 어휘로 인해 의미적 표현의 일부가 손실됩니다. 표현 길이의 차이는 또한 작은 어휘로 단어를 생성하는 비용이 훨씬 더 크다는 것을 의미합니다(2개 대신 9개 토큰이 필요하므로 5배나 더 비용이 많이 듭니다!).

현재 대부분의 사람들은 어휘 크기에 대해 경험적 접근법을 사용하는 것 같습니다. 이는 포함된 언어 수와 모델 크기와 상관관계가 있어 보입니다. 따라서 비슷한 크기의 참조 모델과 가까운 토큰 수를 사용하는 것이 효과적일 수 있습니다.

#### 더 알아보기: 희귀 토큰 효과
- [Less Wrong에서의 SolidGoldMagikarp 포스트](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)
	- 일부 사람들이 OpenAI의 어휘에서 매우 희귀한 토큰을 식별한 방법에 대한 매우 흥미로운 글입니다. 이는 모델의 내부에 접근하지 않고 이루어졌기 때문에 꽤 인상적입니다(예: 학습 데이터에 무엇이 포함되어 있는지 모르는 상태에서)
- [Fishing for Magikarp, Cohere의 논문](https://arxiv.org/abs/2405.05417)
	- 이러한 토큰을 감지하기 위한 후속 연구

### 여러 언어 관리하기
(권장사항: 이 섹션 전에 BPE에 대한 설명을 먼저 읽어보세요)
토크나이저를 구축하거나 선택할 때, 참조 텍스트에서 어휘를 구성합니다. 이는 토크나이저가 이 참조 텍스트의 어휘와 문자를 알게 된다는 의미입니다. 일반적으로 라틴 문자로 된 영어 데이터를 사용하는 것을 의미합니다.

새로운 언어를 추가하고 싶고, 그 언어가 같은 문자 체계를 사용하고 어떤 어원을 공유한다면, 이론적으로 원래 언어의 의미 일부가 새 언어로 전이되기를 기대할 수 있습니다.

그러나 다른 언어(특히 다른 문자 체계로 작성된 언어)에서 텍스트를 올바르게 분할할 수 있도록 하고 싶다면, 해당 토크나이저를 구축할 때 이러한 언어의 데이터를 포함하는 것이 좋습니다. 그러나 대부분의 경우, 이 데이터는 초기 언어(예: 영어)와 새로운 언어(예: 태국어 또는 버마어) 사이에 불균형한 비율을 포함할 것이며, 초기 언어가 훨씬 더 많이 존재합니다. 오늘날 사용되는 대부분의 효율적인 토크나이저 방법(BPE와 같은)은 가장 자주 보이는 단어를 기반으로 복잡한 어휘 토큰을 생성하기 때문에, 대부분의 긴 토큰은 영어 단어가 될 것이고, 덜 빈번한 언어의 대부분의 단어는 문자 수준에서만 분할될 것입니다.

이러한 효과는 다국어 토큰화에서 불공정함으로 이어집니다: 일부 (덜 빈번하거나 *리소스가 적은*) 언어는 영어와 동등한 길이의 문장을 생성하기 위해 훨씬 더 많은 토큰이 필요합니다.

#### 더 알아보기: 언어와 토큰화
- ⭐ [Yennie Jun이 작성한 언어 간 토큰화 문제에 대한 아름다운 분석과 데모](https://www.artfish.ai/p/all-languages-are-not-created-tokenized)
	- 분석 자체가 매우 명확하며, [데모 스페이스](https://huggingface.co/spaces/yenniejun/tokenizers-languages)를 가지고 놀아보는 것도 가치가 있습니다
- ⭐ [Aleksandar Petrov의 토큰화 불공정성에 관한 데모](https://aleksandarpetrov.github.io/tokenization-fairness/)
	- 언어에 따라 추론 비용의 차이를 느껴보려면 `Compare tokenization of sentences`를 보시는 것이 좋습니다

### 숫자는 어떨까요?
토크나이저를 구축할 때, 숫자를 어떻게 처리할지 결정해야 합니다. 0부터 9까지만 인덱싱하고 다른 모든 숫자는 이 숫자들의 조합이라고 가정할 것인지, 아니면 예를 들어 10억까지의 숫자를 개별적으로 저장하고 싶은지 결정해야 합니다. 현재 잘 알려진 모델들은 이에 대해 다양한 접근 방식을 보여주지만, 수학적 추론을 가능하게 하는 데 어떤 방법이 더 잘 작동하는지는 명확하지 않습니다. 아마도 계층적 토큰화와 같은 토큰화에 대한 새로운 접근 방식이 이를 위해 필요할 수 있습니다.

#### 더 알아보기: 숫자 토큰화
- ⭐ [Anthropic, Meta, OpenAI 및 Mistral 모델이 숫자를 어떻게 분할하는지에 대한 Yennie Jun의 시각적 데모](https://www.artfish.ai/p/how-would-you-tokenize-or-break-down) 
- [Beren Millidge가 정리한 수년간 숫자 토큰화의 진화에 관한 간략한 역사](https://www.beren.io/2024-05-11-Integer-tokenization-is-now-much-less-insane/)