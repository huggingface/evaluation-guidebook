# `일부` 평가 데이터셋
## Math specific datasets

| Evaluation name | Task type | Publication date | Data size | Task data  | Task/Paper content  | Source | Dataset  | Comments  |
|-----             |------    |-                 |--         |------------|-------------        |--------|--------  |---------- |
| AGIEval (SATMath) | 시험 데이터셋 + 기존 데이터셋 | 2023 | 220 | SAT에서 가져온 수학 문제 | 논문은 실제로 평가 데이터로 사용할 인간 관련 시험들을 모아놓은 것입니다. | [Paper](https://arxiv.org/abs/2304.06364) | [HuggingFace](https://huggingface.co/datasets/hails/agieval-sat-math) | - 주의, 이 논문은 다른 논문의 데이터셋도 포함합니다! 수학의 경우, MATH 데이터셋을 통한 AIME & AMC, AQuA-Rat 데이터셋을 통한 GRE & GMAT, 그리고 GaoKao를 사용합니다<br>- 평가 지표: acc/em/f1 |
| AIME (all) | 올림피아드 데이터셋 | 1983-현재 | 매년 15 x 2 | 산술, 대수, 계산, 기하학, 수론, 확률 및 기타 중등 수학 주제의 조합이 필요한 수학 문제 | 국제 수학 올림피아드를 위한 미국 팀을 선발하는 2차 시험 | [Blog](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination) | [Source](https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions) | 답은 체계적으로 0에서 999 사이의 정수입니다. |
| AIME (22, 23 and 24) | 올림피아드 데이터셋 | 2024 | 90 <br> | AIME (all) 참조 | | [Paper](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination) | [HuggingFace](https://huggingface.co/datasets/AI-MO/aimo-validation-aime) | AIMO 대회에서 사용됨 |
| ALGES (SingleEQ) | 온라인 소스 컴파일 | 2015 | 508 | 웹 소스에서 추출한 초등학교 대수 문제 | 논문은 문제 뒤에 있는 간단한 방정식을 암묵적으로 학습하고 해결하는 것에 관한 것입니다 | [Paper](https://aclanthology.org/Q15-1042/) | [Source](https://gitlab.cs.washington.edu/ALGES/TACL2015/-/blob/master/questions.json?ref_type=heads) | - 웹 소스: http://math-aids.com, http://k5learning.com, http://ixl.com<br>- LLM 이전 논문 - 데이터 소스는 아마도 괜찮을 것입니다 |
| ALG514 or AllEq | 온라인 포럼 | 2014 | 514 | 크라우드소싱 튜터링 웹사이트에서 추출한 대수 단어 문제, 터킹으로 정리하고 수동으로 검증됨 | 논문은 문제를 해결하기 위해 방정식 템플릿을 추출하는 것에 관한 것입니다 | [Paper](https://aclanthology.org/P14-1026/) | [Source](https://groups.csail.mit.edu/rbg/code/wordprobs/questions.json) | - 웹 소스: Algebra.com |
| AMC 12 | 올림피아드 데이터셋 | 2000 - 현재 | 매년 25 | 산술, 대수, 계산, 기하학, 수론, 확률 및 기타 중등 수학 주제가 필요한 수학 단어 문제. | 국제 수학 올림피아드를 위한 미국 팀을 선발하는 1차 시험, 이전에는 미국 고등학교 수학 시험이었음 | [Blog](https://artofproblemsolving.com/wiki/index.php/AMC_12) | [Source](https://artofproblemsolving.com/wiki/index.php/AMC_12_Problems_and_Solutions) | - 문제는 미적분학 배경 없이도 학생들이 풀 수 있도록 설계되었습니다. |
| Ape210K | 시험 데이터셋 | 2020 | 210K 문제, 56K 템플릿 | 수학 교사가 작성한 중국 초등학교 수준의 수학 단어 문제 | 문제 해결 | [Paper](https://arxiv.org/abs/2009.11506) (철회됨, 그러나 v1은 여전히 접근 가능) | [HuggingFace](https://huggingface.co/datasets/MU-NLPC/Calc-ape210k) | - 일부 문제는 템플릿화되어 있어 오염 문제에 흥미로울 수 있습니다<br>- 초기 데이터셋은 900K이며 수동으로 필터링되었습니다<br>- 또한 "중간 방정식"을 제공합니다(필요한 경우 CoT 추적을 테스트하는 데 유용)<br>- 데이터셋은 중국어로 되어 있습니다<br>- 부분적으로 훈련에 사용하기 위한 것입니다 |
| AQUA or AQUA-Rat | 시험 데이터셋 + 터크 데이터셋 | 2017 | 100K | GMAT 및 GRE의 34K 문제 시드에서 구성된 대수 단어 문제, 터킹을 통해 확장됨 | 작업: 문제 해결 | [Paper](https://arxiv.org/abs/1705.04146) | [HuggingFace](https://huggingface.co/datasets/deepmind/aqua_rat) | - 부분적으로 훈련에 사용하기 위한 것입니다<br>- 문제에 대한 근거를 포함합니다<br>- 점수 매기기에 정확도, BLEU 및 혼란도를 사용합니다... |
| ASDiv-A | 온라인 소스 컴파일 | 2020 | 2.3K | 다양한 웹사이트에서 수집하고 정규화된 수학 세계 초등학교 문제 | 작업: 문제 해결 | [Paper](https://aclanthology.org/2020.acl-main.92/) | [Github](https://github.com/chaochun/nlu-asdiv-dataset) | - 석사 학생 주석자가 주석을 단 문제 유형 및 학년 수준 주석 포함<br>- 높은 어휘 다양성에 중점을 둠<br>- 28개 웹사이트 사용 |
| CHAMP | 올림피아드 데이터셋 | 2024 | 270 | 올림픽 경쟁 예제 책에서 추출한 수학 단어 문제, 해결책을 구문 분석할 수 있도록 다시 작성하고 주석 달기 | 수학 벤치를 소개합니다. 문제는 힌트로 확장되고 개념으로 레이블이 지정되어 성능에 대한 절제 연구를 허용합니다 | [Paper](https://arxiv.org/abs/2406.18321) | | - 출처: "Problem-Solving strategies" 책 (Engel, 2008)<br> |
| DeepMind Math | 시험 데이터셋 + 합성 데이터셋 | 2019 | 10K? | 대수, 산술, 미적분학, 비교, 단위 간 변환, 다항식, 확률 등의 합성 원시 수학 문제. | 작업: 문제 해결 | [Paper](https://arxiv.org/abs/1904.01557) | [HuggingFace](https://huggingface.co/datasets/deepmind/math_dataset) | - 부록 B에 도메인 전체 목록<br>- 논문 첫 번째 섹션이 꽤 좋습니다<br>- 더 많은 예제를 생성하기 위한 생성 코드 제공<br>- 추가 훈련 세트 제공<br>- 합성 절차적 데이터셋(학교 시험 데이터셋에서 영감을 받음/확장?) |
| DocMath-Eval | 주석이 달린 재무 보고서 + 기존 Fin 수학 데이터셋 | 2023 | 3.2K | 재무 보고서와 기존 데이터셋을 결합하고, 주석자가 읽어 질문을 생성(또는 검증)하고, Python 프로그램으로 답변을 제공한 다음 도메인 전문가 주석자로 평가 | 솔루션은 유효성을 테스트하기 위해 실행될 Python 프로그램으로 제시되어야 합니다 | [Paper](https://arxiv.org/abs/2311.09805) | [Source](https://github.com/yale-nlp/docmath-eval) | - TAT-QA, FinQA, MultiHiertt, TAT-HQA 재사용<br>- 수학 금융 데이터에 대해 꽤 높은 품질로 보입니다!<br>- 추가 훈련 세트 제공 |
| Dolphin1878 | 온라인 소스 컴파일 | 2015 | 1.5K | 온라인 소스에서 샘플링하고 필요한 경우 다시 주석이 달린 숫자 수학 단어 문제 | 논문은 의미 구문 분석을 사용하여 문제에서 방정식(DOL) 트리를 추출하는 것에 관한 것입니다 | [Paper](https://aclanthology.org/D15-1135.pdf) | ? | - 소스: algebra.com 및 answers.yahoo.com<br> |
| Dolphin18K | 온라인 소스 컴파일 | 2016 | 18K | 온라인 소스에서 반자동으로 추출된 수학 단어 문제 | | [Paper](https://aclanthology.org/P16-1084.pdf) | [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/sigmadolphin) | - 소스: 2008년 이후 Yahoo 답변의 수학 카테고리<br>- 방법: 수동으로 6K 문제에 주석을 달고 분류기를 사용합니다. 골드를 추출하기 위한 모델 훈련<br>- 자동 추출(높음)과 수동 검증(낮음)의 양을 고려할 때 품질이 놀라울 정도로 좋은지는 확실하지 않습니다 |
| Draw-1K | 온라인 소스 컴파일 | 2016 | 1K | 온라인 소스에서 추출한 일반 대수 단어 문제 | 논문은 솔버(mwp에서 방정식을 생성하는 시스템)를 평가하고 템플릿 및 방정식 동등성을 테스트하는 것에 관한 것입니다 | [Paper](https://arxiv.org/abs/1609.07197) | [Source](https://www.microsoft.com/en-us/download/details.aspx?id=52628) | - 각 문제에 따르는 템플릿으로 레이블을 지정하면 오염에 유용할 수 있습니다<br>- 소스: algebra.com |
| FinQA | 전문가 주석이 달린 재무 보고서 | 2021 | 1.1K | 수익 보고서의 표에 연결된 재무 질문. 주석자는 질문 + 단계별 프로세스 + 각 페이지에 대한 주석을 제공합니다. | 논문은 짧은 컨텍스트 모델을 수용하기 위해 먼저 관련 사실을 추출하는 검색기, 그 다음 프로세스 생성기로 구성된 방법과 함께 데이터셋을 소개합니다. | [Paper](https://arxiv.org/abs/2109.00122) | [HuggingFace](https://huggingface.co/datasets/ibm/finqa) | - 아마도 높은 품질: 유급 전문가 주석자 + 외부 전문가 주석자가 작업에 대해 높은 동의를 보였습니다<br>- 총 세트는 8.2K입니다<br>- 논문에서 표가 어떻게 형식화되었는지 확실하지 않음 - 아마도 마크다운?<br>- 데이터 소스: S&P 500 기업의 수익 보고서(1999-2019) |
| FrontierMath | 전문가 생성 데이터셋 | 2024 | 100+ (정확한 숫자 알 수 없음) | 대부분의 수학 도메인에 걸쳐 논문을 위해 만들어진 완전히 새로운 수학 문제. 솔루션은 단위 테스트와 같은 파이썬 프로그램을 통해 자동으로 검증할 수 있는 정수 또는 SymPy 객체입니다. 문제에는 레이블이 지정되어 있습니다. | 데이터셋 소개 | [Paper](https://arxiv.org/abs/2411.04872) | Private | - 논문에서 오염에 대한 좋은 논의<br>- 전문가: 12개국에 걸친 60명의 수학자<br>- 모든 문제는 동료 검토됨<br>- 아마도 현재 여기서 가장 높은 품질의 데이터셋<br>- 데이터는 공개되지 않았습니다; 그러나 폐쇄 소스 모델이 평가되었기 때문에 향후 발생 시 오염될 가능성이 높습니다 :( |
| GAOKAO-Bench (MathCloze, MathQA) | 시험 데이터셋 | 2023 | ~500 | 중국 대학 입학 시험의 고등학교 수준 수학 단어 문제 | | [Paper](https://arxiv.org/abs/2305.12474) | [Source](https://github.com/OpenLMLab/GAOKAO-Bench?tab=readme-ov-file), <br>데이터셋은 2023년만 있음<br>[HuggingFace](https://huggingface.co/datasets/hails/agieval-gaokao-mathcloze) 및 [HuggingFace](https://huggingface.co/datasets/hails/agieval-gaokao-mathqa) | - 수학 공식은 latex로 변환됨<br>- 문제는 중국어로 되어 있음<br>- 데이터셋은 매년 업데이트됨<br>- 논문은 심판으로서의 LLM을 포함한 다양한 채점 방법을 탐구함<br>- 논문에는 데이터셋에 대한 정보가 놀랍게도 거의 없음 |
| GAOKAO 2023 (MathEn) | 시험 데이터셋, 경쟁 데이터셋 | 2023 | 385 | 고등학교 수준의 수학 단어 문제 | 2023년 중국 전국 대학 입학 시험, 2023년 미국 수학 경시대회, 2023년 미국 대학 시험의 질문을 컴파일합니다 | | [HuggingFace](https://huggingface.co/datasets/MARIO-Math-Reasoning/Gaokao2023-Math-En) | |
| GSM1K | 다른 데이터셋 스타일로 수동 생성된 데이터셋 | 2024 | 1.2K | GSM8K의 해결 분포를 따르는 다양한 "초등학교" 같은 수학 단어 문제 | 논문은 GSM8K와 GSM1K에 대한 모델의 오염 분석을 수행합니다 | [Paper](https://arxiv.org/abs/2405.00332) | Private | - 논문은 또한 혼란도 분석이 오염 감지에 그다지 좋지 않다고 제안하는 것 같습니다 |
| GSM8K | 시험 데이터셋 스타일로 수동 생성된 데이터셋 | 2021 | 8.5K | 다양한 초등학교 수준의 수학 단어 문제 | 논문은 수학 단어 문제를 해결하기 위한 검증기 훈련에 관한 것입니다 | [Paper](https://arxiv.org/abs/2110.14168v2) | [Github](https://github.com/openai/grade-school-math) <br>[Hugging Face](https://huggingface.co/datasets/gsm8k) | - 외부 계산기를 추가한 최상의 결과<br>- 모든 답은 양의 정수이며, 50%의 답은 0에서 8 사이입니다<br>- 주석은 1K 문제에 Upwork를 사용한 다음 Scale을 사용했습니다. 문제 작성자에게는 175B GPT3 모델의 시드 질문이 제공되었습니다 |
| iGSM (med and hard sets) | 합성 데이터셋 | 2024 | 20K | 문제는 객체와 카테고리 간의 의존성 그래프(직접 및 암시적 의존성 포함)와 새로운 문제를 생성하기 위한 작업 수의 조합을 사용하여 생성됩니다 | 논문은 GSM8K의 확장에 대한 실제 수학 추론을 연구하는 것에 관한 것으로, 내부 모델 상태를 탐색하는 것을 포함합니다 | [Paper](https://arxiv.org/pdf/2407.20311) | [HuggingFace](https://huggingface.co/datasets/YangZhoumill/infini_igsm_4k_noise_close) | - 아이디어는 이론적으로 좋지만 생성된 문제는 작업 수가 많아 매우 비현실적입니다<br>- 논문은 내가 의심스럽게 생각하는 모델의 "정신적 과정"에 초점을 맞춥니다(탐색 섹션은 좋습니다!)<br>- 너무 많은 의인화 -_- |
| GSMHard | 기존 데이터셋 적응, 숫자 대체 | 2022 | 8.5K | 문제를 더 어렵게 만들기 위해 더 크거나 덜 일반적인 숫자가 있는 GSM8K. 그러나 변경은 생성된 프로그램을 통해 자동으로 수행되었으며, 25개의 변경만 확인되었습니다(+ 50개 사례는 수동으로 수행됨). | 논문은 프로그램 지원 LM 사용에 관한 것입니다(= 방정식과 추론 단계를 번갈아 가며 CoT를 생성하고 Python으로 마지막 방정식에서 해결책을 계산) | [Paper](https://arxiv.org/abs/2211.10435) | [Hugging Face](https://huggingface.co/datasets/reasoning-machines/gsm-hard) | - 부록 H1에 설명됨<br>- 좋은 아이디어이지만 품질이 확실하지 않습니다. |
| GSM-IC | 교란이 있는 기존 데이터셋 적응 | 2023 | 58K | 관련 없는 컨텍스트가 추가된 GSM8K의 100개 샘플(관련 없는 문장에 대한 템플릿과 역할/숫자 필러 사용) | 논문은 수학 작업에서 추론할 때 LLM이 관련 없는 컨텍스트에 얼마나 민감한지 테스트합니다 | [Paper](https://arxiv.org/abs/2302.00093) | [HuggingFace](https://huggingface.co/datasets/voidful/GSM-IC) | |
| GSM-Plus | 교란이 있는 기존 데이터셋 적응 | 2024 | 10K | GPT4에 의해 추가되고 선택된 인간에 의해 수동으로 주석이 달린 질문당 8개의 변형이 있는 GSM8K(주석자 간 교차 동의 확인됨) | 논문은 데이터셋을 소개하고 여러 모델과 프롬프팅 형식에 걸쳐 여러 GSM8K 변형에 대한 결과를 비교합니다 | [Paper](https://aclanthology.org/2024.acl-long.163/) | [HuggngFace](https://huggingface.co/datasets/qintongli/GSM-Plus) | - 변경 사항에는 다음이 포함됩니다: 숫자를 다른 숫자로 대체, 작업 변경, 질문 변경, 방해 요소 추가 등(변경 사항의 좋은 유형학, 확장될 수 있다고 생각함) |
| GSM-Symbolic | 템플릿화된 기존 데이터셋 적응 | 2024 | 8.5K | 템플릿화된 GSM8K 문제로, 원하는 대로 새로운 평가를 생성할 수 있습니다 | 논문은 GSM8K에서 구문 분석 가능한 템플릿을 만들어 원하는 대로 새로운 문제를 생성하고 GSM8K의 오염을 분석할 수 있게 합니다 | [Paper](https://arxiv.org/abs/2410.05229) | 출시 예정 | - 다른 특정 하위 집합(M1, P1, P2, 난이도 수준, 그리고 NoOp, 관련이 있는 것처럼 보이지만 실제로는 관련 없는 정보가 추가된)을 포함하며, 일부 실험은 few shot 형식으로 수행됩니다<br>- 모든 하위 집합이 있는 데이터셋 설명 표가 부족하다고 생각합니다 |
| Hungarian HighSchool National Finals Exam | 시험 데이터셋 | 2023 | 33 | 2023년 헝가리 국가 고등학교 수학 결승의 문제 | | [Source](https://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/k_matang_23maj_fl.pdf) | [HuggingFace](https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam) | - 현재 수동 채점이 필요합니다 |
| HMWP | 시험 데이터셋 | 2020 | 5.4K | 중국 학교 수준(K-12) 문제 은행에서 주석이 달린 수학 단어 문제 | MWP 방정식을 균일하게 표현하기 위한 새로운 형식주의를 소개합니다 | [Paper](https://arxiv.org/abs/2010.06823) | [HuggingFace](https://huggingface.co/datasets/Gxg/HWMP) | - 데이터셋은 중국어로 되어 있습니다<br>- 소스: 중국 K12 문제 |
| Math23K | 온라인 소스 컴파일 | 2017 | 23K | 자동으로 추출된 초등학교 수준의 수학 단어 문제. | MWP를 해결하기 위한 RNN을 소개합니다 | [Paper](https://aclanthology.org/D17-1088/) | [HuggingFace](https://huggingface.co/datasets/Gxg/Math23K) | - 소스: 초등학생을 위한 온라인 교육 웹사이트의 중국 수학 단어 문제.<br>- 데이터셋은 중국어로 되어 있습니다<br>- 추출은 규칙 기반이지만 수동 검증이 얼마나 이루어졌는지 매우 불분명합니다 |
| Math401-LLM | 합성 데이터셋 | 2023 | 401 | 덧셈, 뺄셈, 곱셈, 지수, 로그 등을 결합한 산술 표현 | 논문은 모델의 엄격한 산술 능력을 측정하고자 합니다 | [Paper](https://arxiv.org/abs/2304.02015) | [Github](https://github.com/GanjinZero/math401-llm) | - 모델은 현재 로그/삼각 문제나 큰 숫자에 대해 그렇게 좋지 않습니다 |
| AGIEval (SATMath) | Exam dataset + existing datasets  | 2023 | 220 | Math problems from the SAT | Paper is actually a compilation of a bunch of human relative exams to use as eval data. | [Paper](https://arxiv.org/abs/2304.06364)  | [HuggingFace](https://huggingface.co/datasets/hails/agieval-sat-math)  | - Careful, this paper also includes datasets from other papers! For math, they use AIME & AMC through the MATH dataset, GRE & GMAT through the AQuA-Rat dataset, and GaoKao<br>- Metrics: acc/em/f1 |
| AIME (all)  | Olympiad dataset  | 1983-now | 15 x 2  per year  | Mathematical problems requiring a combination of arithmetic, algebra, counting, geometry, number theory, probability and other secondary school math topics  | 2nd exam to choose the US team for the International Math Olympiads | [Blog](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination) | [Source](https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions) | Answer is systematically an integer between 0 and 999.  |
| AIME (22, 23 and 24)  | Olympiad dataset  | 2024 | 90 <br> | See AIME (all) | | [Paper](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination)  | [HuggingFace](https://huggingface.co/datasets/AI-MO/aimo-validation-aime)  | Used in the AIMO competition  |
| ALGES (SingleEQ)  | Online sources compilations | 2015 | 508 | Grade school algebra problems extracted from sources in the web  | Paper is about implicitely learning and solving the simple equation behind the problem  | [Paper](https://aclanthology.org/Q15-1042/)  | [Source](https://gitlab.cs.washington.edu/ALGES/TACL2015/-/blob/master/questions.json?ref_type=heads)  | - Web sources:  http://math-aids.com, http://k5learning.com, and http://ixl.com<br>- Pre-LLM paper - data sources are probably OK |
| ALG514 or AllEq | Online forum  | 2014 | 514 | Algebra word problems extracted from a crowdsourced tutoring website, cleaned with turking, and manually verified  | Paper is about extracting the equation template from the problem to solve it  | [Paper](https://aclanthology.org/P14-1026/)  | [Source](https://groups.csail.mit.edu/rbg/code/wordprobs/questions.json) | - Web source: Algebra.com |
| AMC 12  | Olympiad dataset  | 2000 - now | 25 per year | Mathematical word problems requiring arithmetic, algebra, counting, geometry, number theory, probability and other secondary school math topics. | 1st exam to select the US team for the International Math Olympiad, used to be the Americal High School Math exam | [Blog](https://artofproblemsolving.com/wiki/index.php/AMC_12)  | [Source](https://artofproblemsolving.com/wiki/index.php/AMC_12_Problems_and_Solutions) | - Problems are designed to be solvable by students without any background in calculus.  |
| Ape210K | Exam dataset  | 2020 | 210K problems, 56K templates  | Chinese elementary school-level math word problems written by math teachers  | Solve the problems  | [Paper](https://arxiv.org/abs/2009.11506) (withdrawn, but v1 still accessible) | [HuggingFace](https://huggingface.co/datasets/MU-NLPC/Calc-ape210k)  | - Some problems are templated, which could be interesting for contamination issues<br>- Initial dataset is 900K and was manually filtered<br>- Also provides "intermediate equations" (useful to test CoT traces if needed)<br>- Dataset is in Chinese<br>- Intended to be partially used for training  |
| AQUA or AQUA-Rat  | Exam dataset + turked dataset | 2017 | 100K  | Algebraic word problems constructed from a seed of 34K problems from GMAT and GRE, and extended via turking  | Task: Solve the problem | [Paper](https://arxiv.org/abs/1705.04146)  | [HuggingFace](https://huggingface.co/datasets/deepmind/aqua_rat) | - Intended to be partially used for training<br>- Includes the rationale for the problems<br>- Use accuracy, BLEU and perplexity for scoring ...  |
| ASDiv-A | Online sources compilation  | 2020 | 2.3K  | Math world grade-school problems collected from various websites and normalized  | Task: Solve the problem | [Paper](https://aclanthology.org/2020.acl-main.92/)  | [Github](https://github.com/chaochun/nlu-asdiv-dataset)  | - Contains problem type and grade level annotations by a Master's student annotator<br>- Focused on a high lexical diversity<br>- Used 28 websites  |
| CHAMP | Olympiad dataset  | 2024 | 270 | Math word problems extracted from a book of olympic competitions examples, rewritten to make the solutions parsable, and annotated | Introduces a math bench. Problems are extended with hints and labeled with concepts, to allow ablation studies on performance | [Paper](https://arxiv.org/abs/2406.18321)  |  | - Source: Book "Problem-Solving strategies" (Engel, 2008)<br> |
| DeepMind Math | Exam dataset + synthetic dataset  | 2019 | 10K?  | Synthetic raw math problems in algebra, arithmetic, calculus, comparision, conversions between units, polynomials, probability, etc. | Task: Solve the problem | [Paper](https://arxiv.org/abs/1904.01557)  | [HuggingFace](https://huggingface.co/datasets/deepmind/math_dataset) | - Full list of domains in appendix B<br>- Paper first section is quite nice<br>- Provide generation code to generate more examples<br>- Provides additional train set<br>- Synthetic procedural dataset (inspired from/to extend? school exams dataset) |
| DocMath-Eval  | Annotated financial reports + exisiting Fin math datasets | 2023 | 3.2K  | Combine financial reports and existing datasets, read by annotators to generate (or validate) questions, and provide answers as Python programs, then evaluated with domain expert annotators  | Solutions should be presented as Python programs which will be run to test their validity | [Paper](https://arxiv.org/abs/2311.09805)  | [Source](https://github.com/yale-nlp/docmath-eval) | - Re-uses TAT-QA, FinQA, MultiHiertt, TAT-HQA<br>- Looks quite high q for math fin data!<br>- Provides additional train set |
| Dolphin1878 | Online sources compilations | 2015 | 1.5K  | Number math word problems sampled from online sources and re-annotated if needed | Paper is about extracting the equation (DOL) tree from the problem using semantic parsing | [Paper](https://aclanthology.org/D15-1135.pdf) | ?  | - Sources: algebra.com and answers.yahoo.com<br>  |
| Dolphin18K  | Online sources compilations | 2016 | 18K | Math word problems semi automatically extracted from online sources  | | [Paper](https://aclanthology.org/P16-1084.pdf) | [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/sigmadolphin)  | - Sources: Math category of Yahoo answers since 2008<br>- Method: manually annotate 6K problems then use a classifier. Train a model to extract the gold<br>- I'm not sure the quality is amazing there given the amount of automatic extraction (high) vs manual verif (low) |
| Draw-1K | Online sources compilations | 2016 | 1K  | General algebra word problems extracted from online sources  | Paper is about evaluating solvers (systems generating equations from mwp), and test template and equation equivalence | [Paper](https://arxiv.org/abs/1609.07197)  | [Source](https://www.microsoft.com/en-us/download/details.aspx?id=52628) | - Label each problem with the template it follows, can be useful for contam<br>- Source: algebra.com  |
| FinQA | Expert annotated financial reports  | 2021 | 1.1K  | Financial questions linked to tables from earning reports. Annotators provide a question + step by step process + annotations for each page. | Paper introduces the dataset plus a method made of a retriever which extracts relevant facts first to accomodate short context models, then a process generator.  | [Paper](https://arxiv.org/abs/2109.00122)  | [HuggingFace](https://huggingface.co/datasets/ibm/finqa) | - Likely high quality: used paid expert annotators + external experts annotators had high agreement on the task<br>- Total set is 8.2K<br>- Unsure from the paper how the tables are formatted - markdown maybe?<br>- Data source: earnings reports of S&P 500 companies (1999-2019)  |
| FrontierMath  | Expert created datasset | 2024 | 100+ (precise number unknown) | Entirely novel math problems created for the paper across most math domains. Solutions are either integer or SymPy objects to be automatically verifiable through unit-test like python programs. Problems are labelled. | Introduces the dataset  | [Paper](https://arxiv.org/abs/2411.04872)  | Private  | - Nice discussion of contamination in the paper<br>- Experts: 60 mathematicians over 12 countries<br>- All problems are peer reviewed<br>- Probably the highest quality dataset here atm<br>- Data is not public; however, since closed source models have been evaluated, it's likely they'll be contaminated for it in future occurences :( |
| GAOKAO-Bench (MathCloze, MathQA)  | Exam dataset  | 2023 | ~500  | Math word problems at high school level from the Chinese college entry exams | | [Paper](https://arxiv.org/abs/2305.12474)  | [Source](https://github.com/OpenLMLab/GAOKAO-Bench?tab=readme-ov-file), <br>Datasets are only 2023<br>[HuggingFace](https://huggingface.co/datasets/hails/agieval-gaokao-mathcloze) and [HuggingFace](https://huggingface.co/datasets/hails/agieval-gaokao-mathqa) | - Mathematical formulas are converted to latex<br>- Problems are in Chinese<br>- Dataset is updated yearly<br>- Paper explores a bunch of grading methods, including LLM as judge<br>- Paper contains surprinsingly little info about the dataset |
| GAOKAO 2023 (MathEn)  | Exam dataset, Competition dataset | 2023 | 385 | Math word problems at high school level  | Compiles questions from the 2023 Chinese National College Entrance Examination, the 2023 American Mathematics Competitions, and the 2023 American College Testing |  | [HuggingFace](https://huggingface.co/datasets/MARIO-Math-Reasoning/Gaokao2023-Math-En) | |
| GSM1K | Manually created dataset in the style of another dataset  | 2024 | 1.2K  | Diverse "grade school"-like math word problems, following the solving distribution of GSM8K  | Paper does a contamination analysis of models on GSM8K vs GSM1K | [Paper](https://arxiv.org/abs/2405.00332)  | Private  | - Paper also seems to suggest that perplexity analysis is not very good at detecting contamination  |
| GSM8K | Manually created dataset in the style of an exam dataset  | 2021 | 8.5K  | Diverse grade school-level math word problems  | Paper is about training verifiers to solve math word problems | [Paper](https://arxiv.org/abs/2110.14168v2)  | [Github](https://github.com/openai/grade-school-math) <br>[Hugging Face](https://huggingface.co/datasets/gsm8k)  | - Best results with an external calculator added<br>- All answers are positive integers, 50% of answers are between 0 and 8<br>- Annotation used Upwork for 1K problems, then Scale for the next. Problems writers were provided with seed questions from a 175B GPT3 model |
| iGSM (med and hard sets)  | Synthetic dataset | 2024 | 20K | Problems are generated using a combination of dependency graphs between objects and categories (with direct, and implicit dependencies) and number of operations to generate new problems  | Paper is about studying actual math reasoning on an extension of GSM8K, including probing internal model states | [Paper](https://arxiv.org/pdf/2407.20311)  | [HuggingFace](https://huggingface.co/datasets/YangZhoumill/infini_igsm_4k_noise_close) | - Idea is theoretically nice but problems generated are very unrealistic with high numbers of operation<br>- Paper focuses on "mental process" of model which I find dubious (though the probing section is nice!)<br>- So much anthropomorphism -_-  |
| GSMHard | Adaptation of existing dataset, numbers replaced  | 2022 | 8.5K  | GSM8K with bigger/less common numbers to make the problems harder. However, change was done automatically through programs generated, and only 25 changes were checked (+ 50 cases were done manually).  | Paper is about using program-aided LMs (= generating CoT alternating equations and reasoning steps, and computing the solution on the last equation with Python)  | [Paper](https://arxiv.org/abs/2211.10435)  | [Hugging Face](https://huggingface.co/datasets/reasoning-machines/gsm-hard)  | - Described in appendix H1<br>- Good idea, but not sure of the quality. |
| GSM-IC  | Adaptation of existing dataset with perturbations | 2023 | 58K | 100 samples from GSM8K with irrelevant context added (using a template for the irrelevant sentence, plus roles/numbers fillers)  | Paper tests how sensitive LLMs are to irrelevant context when reasoning on math tasks | [Paper](https://arxiv.org/abs/2302.00093)  | [HuggingFace](https://huggingface.co/datasets/voidful/GSM-IC)  | |
| GSM-Plus  | Adaptation of existing dataset with perturbations | 2024 | 10K | GSM8K with 8 variations per question, added by GPT4 and manually annotated by selected humans (cross annotator agreement checked)  | Paper introduces the dataset and compares results on several GSM8K variants across models and prompting formats | [Paper](https://aclanthology.org/2024.acl-long.163/) | [HuggngFace](https://huggingface.co/datasets/qintongli/GSM-Plus) | - Changes include: replacing numbers by ohter numbers, changing the operations, changing the question, adding distractors, etc (nice typology of changes, I feel it could be extended)  |
| GSM-Symbolic  | Adaptation of existing dataset, templated | 2024 | 8.5K  | Templated GSM8K problems, which allows to generate new evals at will | Paper creates parsable templates from GSM8K to be able to generate new problems at will and analyse contamination on GSM8K  | [Paper](https://arxiv.org/abs/2410.05229)  | To be released | - Contains other specific subsets (M1, P1, P2, which are difficulty levels, as well as NoOp, with seemingly relevant but actually irrelevant info added), and some experiments are done with few shot formats<br>- Lacking a dataset description table with all subsets imo |
| Hungarian HighSchool National Finals Exam | Exam dataset  | 2023 | 33  | Problems from the 2023 hungarian national high school finals in math | | [Source](https://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/k_matang_23maj_fl.pdf) | [HuggingFace](https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam) | - Require grading by hand atm |
| HMWP  | Exam dataset  | 2020 | 5.4K  | Annotated math word problems from a Chinese school-level (K-12) problems bank  | Introduces a new formalism to represent MWP equations uniformly | [Paper](https://arxiv.org/abs/2010.06823)  | [HuggingFace](https://huggingface.co/datasets/Gxg/HWMP)  | - Dataset is in Chinese<br>- Sources: Chinese K12 problems  |
| Math23K | Online sources compulation  | 2017 | 23K | Automatically extracted elementary school level math word problems.  | Introduces a RNN to solve MWP | [Paper](https://aclanthology.org/D17-1088/)  | [HuggingFace](https://huggingface.co/datasets/Gxg/Math23K) | - Sources: Chinese math word problems from online education websites for elementary school students.<br>- Dataset is in Chinese<br>- Extraction is rule based, but it's very unclear how much manual validation was done  |
| Math401-LLM | Synthetic dataset | 2023 | 401 | Arithmetic expressions combining additions, substractions, multiplications, exponentiations, logarithms etc  | Papers wants to measure strict arithmetic ability of models | [Paper](https://arxiv.org/abs/2304.02015)  | [Github](https://github.com/GanjinZero/math401-llm)  | - Models are not that good atm for log/trig problems or big numbers |
| MATH  | Olympiad datasets | 2021 | 12.5K | Mathematical problems from real competitions in natural language and latex, annotated with difficulty levels.  | | [Paper](https://arxiv.org/abs/2103.03874)  | [HuggingFace](https://huggingface.co/datasets/lighteval/MATH)  | - Sources: AMC 10, AMC12, AOME, "and more"<br>- Also introduces a train set created from scraping Khan academy and AMPS |
| MathOdyssey | |  | |  | |  |  | |
| MathQA  | Adaptation of existing dataset, annotated | 2019 | 37K | Annotated solvable problems from the AQuA dataset with formal annotation programs (using humans as annotators and testing their agreement) | Aims to introduce a representation language for math problems, applies the method to AQuA | [Paper](https://arxiv.org/abs/1905.13319)  | [HuggingFace](https://huggingface.co/datasets/allenai/math_qa) | - Sources: AQuA |
| MAWPS | Existing dataset compilation  | 2016 | 3.3K  | Math world problems from existing datasets | Framework to create new math problems, notably to remove lexical or template overlap when adding new datasets | [Paper](https://aclanthology.org/N16-1136/)  | [Github](https://github.com/sroy9/mawps) | - Sources: ALG514, ALGES, and other pre-LLM datasets  |
| MiniF2F | Olympiad dataset  | 2022 | 244 | Olympiad math word problems formalized with theorem provers when possible (Lean, Methamath, Isabelle)  | Paper is about testing math proof solvers ability to reason on formal logic | [Paper](https://arxiv.org/abs/2109.00110)  | Possibly [HuggingFace](https://huggingface.co/datasets/cat-searcher/minif2f-lean4) | - Sources: AIME, AMC, IMO |
| NPHardEval  | Synthetic dataset | 2023 | 900 | Complexity math word problems of varied difficulty level built from synthetic graph/linear data  | Paper introduces the benchmark and uses it to evaluate reasoning ability of models. Also explores benchmark robustness! | [Paper](https://arxiv.org/abs/2312.14890)  | [Github](https://github.com/casmlab/NPHardEval)  | - Problems: sorted array search, edit distance, shortest path, traveling salesman, graph coloring, knapsack problem, meeting scheduling problem<br>- Can be regenerated as needed |
| NuminaMATH CoT  | Existing dataset compilation  | 2024 | 860K  | Math word problems (K12 + olympiad levels) combining existing datasets | NA  | NA | [HuggingFace](https://huggingface.co/datasets/AI-MO/NuminaMath-CoT)  | - Sources: AOPS, AMC, AIME, CN-K12, GSM8K, MATH, ORCA_math, Synthetic AMC and MATH data, and other Olympiads sets<br>- careful if you use this as train set as you will be contaminated on all major math bencks  |
| NuminaMATH TiR  | Existing dataset compilation  | 2024 | 72K | Subset of NuminaMATH CoT focused on problems solvable with tool integrated reasoning | NA  | NA | [HuggingFace](https://huggingface.co/datasets/AI-MO/NuminaMath-TiR)  | - Sources: AOPS, AMC, AIME, CN-K12, GSM8K, MATH, ORCA_math, Synthetic AMC and MATH data, and other Olympiads sets<br>- careful if you use this as train set as you will be contaminated on all major math bencks  |
| OmniMath  | Olympiad datasets | 2024 | 2.2K  | Olympiad math word problems. Problems are extracted from forums or olympiad websites (using rule based + LLM rephrasing), then annotated and verified by humans. | Paper introduces the benchmark and a judge trained to evaluate the answers (since they are free form) | [Paper](https://arxiv.org/abs/2410.07985)  | [HuggingFace](https://huggingface.co/datasets/KbsdJames/Omni-MATH) | - Sources: IMO, IMC, AoPS forum and wiki<br>- Domain labeling is done with LLMs |
| OlympiadBench | Olympiad datasets | 2024 | 8.4K  | Olympiad/math/physics word problems. Answers are automatically evaluated - either numbers or equations (evaluated with SymPy)  | | [Paper](https://arxiv.org/pdf/2402.14008)  |  | - Sources: Global Mathematics and Physics Olympiad Problems, Regional and National Chinese Math Competitions, and Gaokao Mock Questions for Mathematics and Physics<br>- Includes a physics subset<br>- VLM evaluation! |
| OlympicArena  | Olympiad datasets | 2024 | 11K |  | | [Paper](https://arxiv.org/pdf/2406.12753)  |  | |
| PRM800K | Synthetic data  | 2023 | 800K  | Preference data from annotators on 800K solutions generated by a model | Paper introducing process supervision to improve reward models (compares output and process supervision)  | [Paper](https://arxiv.org/abs/2305.20050)  | [HuggingFace](https://huggingface.co/datasets/tasksource/PRM800K)  | - More a train set than an evaluation |
| SVAMP | Adaptation of existing dataset  | 2021 | 1K  | One-unknown arithmetic word problems of<br>grade level up to 4, created with experts applying variations to ASDiv-A. | Paper wants to assess question sensitivitiy, reasoning ability, and structure invariance in models for math evaluations.  | [Paper](https://aclanthology.org/2021.naacl-main.168/) | [Github](https://github.com/arkilpatel/SVAMP/blob/main/SVAMP.json) | - Variations: same object & different structure, opposite, both different, adding relevant or irrelevant information, changing information, inverting operations, changing order of sentences or objects  |
| TabMWP  | Online source adaptation  | 2022 | 38K | Tabular math word problems requiring multi-hop reasoning, extracted from an online educative website and manually annotated. | Paper wants to test tabular math reasoning, datast  | [Paper](https://arxiv.org/abs/2209.14610)  | [HuggingFace](https://huggingface.co/datasets/Arietem/tabmwp)  | - Source: IXL learning website<br>- Tabular data is provided as an image, semi-structured text, and a table<br>- Answers are generative or MCQA<br>- Dataset is tested against turkers  |
| TAL-SCQ5K-En  | Competitions dataset  | 2023 | 4K  | Math word problems in MCQA format, with math expressions as latex  | NA  | None | [HuggingFace](https://huggingface.co/datasets/math-eval/TAL-SCQ5K) | - Contains English and Chinese<br>- Also contains 6K train samples and CoT  |
| TemplateGSM | LLM-generated data  | 2024 | 7M  | GPT4-generated math word problems inspired in shape by GSM8K | Paper uses GPT4 generated meta-template to generate problems by changing parameters. Uses a verificator to ensure usability | [Paper](https://templatemath.github.io/TemplateMath_Part_I.pdf)  | [HuggingFace](https://huggingface.co/datasets/math-ai/TemplateGSM) | - Since everything is LLM generated, I would expect stronger proofs of quality  |
| TheoremQA | Online sources adaptations  | 2023 | 800 | QAs about university level theorems  | Protocol: Uses GPT4 to enumerate subfields of relevant domains, then plausible theorems lists, then uses domain experts to actually look for said theorems, then look for QA on the web concerning them | [Paper](https://arxiv.org/abs/2305.12524)  | [HuggingFace](https://huggingface.co/datasets/TIGER-Lab/TheoremQA) | |

## Pre-LLM datasets

| Evaluation name | Task type | Task data | Task content | Source | Dataset | Comments |
|--- |--- |--- |--- |--- |--- |--- |
| DeepFix | Code task, Code-to-code, Correction | 7K student-written erroneous C programs | Correct the C programs | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/10742) | | |
| MLSum | Generation, Multilingual, Summarization | 1.5M news summary/article pairs from the DailyMail, Le Monde, Süddeutsche Zeitung, El Pais, Moskovskij Komsomolets and Internet Haber (en, fr, de, es, ru, tur) | Summarize the articles | [Paper](https://arxiv.org/abs/2004.14900) | [Hugging Face](https://huggingface.co/datasets/mlsum) | Palm: Prefixed with a prompt, truncated article to 2048 tokens |
| TransCoder | Code task, Code-to-code | 852 parallel functions in Python/Java/C++ | Translate from a language to another | [Paper](https://arxiv.org/pdf/2006.03511.pdf) | [From paper](https://github.com/facebookresearch/CodeGen/blob/main/docs/transcoder.md) | |
| WMT | Multilingual, Translation | Datasets from the WMT conf on machine translation - datasets available depend on the year | Translate from a language to another | [Conference](https://www.statmt.org/wmt20/) <br>Replace the 2 digits by the conference year | | |
| Adversarial NLI | Language Inference | 10K entailment dataset generated using human in the loop adversarial attacks, looking for predicates which force models to predict wrong entailement labels (uses contexts from StoryCloze, CommonCrawl, Wikipedia, the Open Annotated National Corpus, WikiHow and GLUE) | Predict entailment | [Paper](https://arxiv.org/abs/1910.14599) | [Data](https://dl.fbaipublicfiles.com/anli/anli_v1.0.zip )<br>[Github](https://github.com/facebookresearch/anli) | R1 to R3 = rounds of data generation |
| APPS | Text-to-code| 10K Python coding problems in natural languages, scraped from leetcode sites, with a suite of test cases. | Solve the Python problem | [Paper](https://arxiv.org/abs/2105.09938) | [Github](https://github.com/hendrycks/apps) <br>[Data](https://people.eecs.berkeley.edu/~hendrycks/APPS.tar.gz) | |
| AQuA | Arithmetic, Reasoning | 100K multiple choice problems (GMAT, GRE, other sources) with question/options/rationale | Select the correct MCQA | [Paper](https://arxiv.org/abs/1705.04146) | [Github](https://github.com/deepmind/AQuA) | Best results obtained with an external calculator added |
| ARC | Common Sense, Reasoning | 8K Grade school science questions: e = easy set, c = challenge set | Select the correct MCQA | [Paper](https://arxiv.org/abs/1803.05457) | [Data](https://allenai.org/data/arc) | Careful, this is the AI2 Reasoning Challenge, not the Abstraction and Reasoning Corpus |
| bAbI | Reasoning | 20 tasks each with 2K automatically generated questions + short scenarios (successive actions generated with a simulated text adventure game). | Reason over the sentence to select the correct conclusion | [Paper](https://arxiv.org/abs/1502.05698) | [Github](https://github.com/facebookarchive/bAbI-tasks) <br>[Data](https://research.facebook.com/downloads/babi/) | See Part 4 for the simulation env and its constraints, it's quite a fun idea. Probably not too hard to reproduce for other types of reasoning. |
| BBQ | Bias detection | 58K examples with two contexts (ambiguous and explicit about a bias), two questions (negative and non-negative) and possible answers, constructed from manual templates and checked with crowdsourcing. | Predict the correct, non biased answer. Difference between accuracies depending on context/question allows to build a bias score. | [Paper](https://aclanthology.org/2022.findings-acl.165/) | [Github](https://github.com/nyu-mll/BBQ/tree/main/data) | |
| BLiMP | Language Understanding | 67 datasets of each artificially generated 1K minimal pairs testing syntactic, morphological and semantic knowledge, validated with MTurking. | Accuracy measured by looking if the log-probability the model assigned to the correct sentence is higher. | [Paper](https://aclanthology.org/2020.tacl-1.25/) | [Github](https://github.com/alexwarstadt/blimp/tree/master/data) | Things tested: anaphor agreement, argument structure, binding, control/raisong, determiner-noun agreement, ellipsis, filler-gap, irregular forms, island effects, NPI licensing, quantifiers, subject-verb agreement |
| BOLD | Generation, Toxicity detection | 23K prompts extracted from beginning of Wikipedia sentences containing a race/religion/political/gender/profession group member (ex: woman artist for gender=female). <br> | Task: generating end of sentence, and toxicity is evaluated through a range of metrics (sentiment analysis, using classifiers, …). In HELM, toxicity is measured using the Perspective API.<br> | [Paper](https://arxiv.org/abs/2101.11718) | [Github](https://github.com/amazon-science/bold/tree/main/prompts) | |
| BooksCorpus | N/A | 11K unpublished books of more than 20K words scraped from the web (of 16 different genres). <br><br> | In the original paper, it's used to train a sentence embedding model - in model papers, it's often used for contamination or perplexity evaluations. | [Paper](https://arxiv.org/pdf/1506.06724.pdf) | [Hugging Face](https://huggingface.co/datasets/bookcorpus) | |
| BooksCorpus_HELM| Generation, Memorisation | 1K randomly sampled books from BooksCorpus. | Task: from a random number of tokens beginning a paragraph, the model must generate a follow up - measure exact and near-exact reproduction. | [Paper](https://arxiv.org/abs/2211.09110) | [Data](https://drive.google.com/file/d/10uC4jM6tgI1pgtq--07FFHQ2Te7-SXGA/view) | |
| BoolQ | Language Inference, Language Understanding | 16K sentences of naturally occurring yes no QA, from question + context from Wikipedia | Answer the MCQA | [Paper](https://arxiv.org/abs/1905.10044) | [Website](https://super.gluebenchmark.com/tasks) | |
| CB | Language Understanding | 1.2K of discourse (news from the Wall Street Journal, fiction from the British National Corpus, dialogue from Switchboard), containing context + target sentence | Predict commitment entailment | [Paper](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf) | [Website](https://super.gluebenchmark.com/tasks) | |
| Civil comments | Toxicity detection | 1.8M online comments, with crowd sourced human labels for and toxicity following the Perspective API guidelines, and among these, 450K labeled with identity terms (crowdsourced, to pick in a list). | Task : toxicity prediction, labels are used to identify areas of biases in models | [Paper](https://arxiv.org/pdf/1903.04561.pdf) | [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)<br>[Hugging Face](https://huggingface.co/datasets/civil_comments) | Original paper contained a synthetic test set (77K examples generated from templates using 50 identity terms, 50/50 on toxicity) and a human labeled dataset (description in the Task content col) - I suppose the dataset available is the latter |
| Clean E2E NLG | Description, Generation | 50K crowdsourced generated descriptions of restaurants given keys and values (type of food = X, budget = Y, …). | | [Paper](https://arxiv.org/abs/1706.09254) | [Hugging Face](https://huggingface.co/datasets/e2e_nlg_cleaned) | Palm: Prefixed with a prompt, truncated article to 2048 tokens |
| CNN/DailyMail | Cloze/Completion, Summarization | Original dataset: 200K new documents (CNN/DailyMail between 2007 and 2015) converted into Cloze format by removing some of the text's named entities, and using them as keys. <br> | In HELM: Uses above documents (in complete form) as text to summarize, and their highlights as gold summaries. | [Paper](https://arxiv.org/pdf/1506.03340.pdf) | [Hugging Face](https://huggingface.co/datasets/cnn_dailymail) <br>[Data](https://cs.nyu.edu/~kcho/DMQA/) | (I suspect this does not produce very good summaries though) |
| CommonsenseQA | Common Sense, Reasoning | 12K turked Q/A (initialized from ConceptNet associations), then filtered by quality, with added context from a Google Search query > Some text likely overlaps with CC data | Answer the MCQA | [Paper](https://aclanthology.org/N19-1421/) | | Best results with an external calculator added |
| Contrast Sets | Generation, Robustness | 10 contrast sets of up to 1K examples, for their datasets (see comments) made by (often the original paper's) researchers (increase reasoning steps in questions, replace words by their opposites, change counts …). | Follow original task setup with new examples, and see how/if performance drops. <br>In HELM: use the IMDb and DROP contrast sets | [Paper](https://aclanthology.org/2020.findings-emnlp.117/) | [Data](https://allenai.org/data/contrast-sets) | NLVR2, IMDb sentiment analysis, MATRES Temporal RE, English UD parsing, PERSPECTRUM, DROP, Quoref, ROPES, BoolQ, MC-TACO. <br> <br>Dataset construction is way more detailed in Appendix |
| COPA | Common Sense, Language Understanding | 1K premise + causal questions with alternatives (common sense) | | [Paper](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF) | [Website](https://super.gluebenchmark.com/tasks) | |
| CoQA | In-Context Reading Comprehension | 127K Conversational QA, from a given context (rationale must be provided too) - written by annotators | | [Paper](https://arxiv.org/abs/1808.07042) | v1.0 from [Data](https://stanfordnlp.github.io/coqa/) | |
| DataImputation | Real life task, Reasoning, Structured data | 8 structured datasets from several sources. | Task: from a row of attributes with gaps, the model must fill the gaps (ex: extrapolating city from phone number, phone brand from its specs). <br> | [Paper](https://sxsong.github.io/doc/21icde-imputation.pdf) | [Data restaurant](https://www.cs.utexas.edu/users/ml/riddle/data/restaurant.tar.gz)<br>[Data Buy](https://dbs.uni-leipzig.de/file/Abt-Buy.zip) | See table 2 for all sources.<br>In HELM: use the subsets Buy and Restaurant, convert input to natural language, test accuracy.|
| Digits arithmetics (2D+, 2D-, 3D+, 3D-, …) | Arithmetic | Basic arithmetic tasks for n digits addition, subtraction, composite operations with 2K example each | Task: solve the math | [Paper](https://arxiv.org/pdf/2005.14165.pdf) | [Github](https://raw.githubusercontent.com/openai/gpt-3/master/data/) | All links come from the lm-evaluation-harness/lm_eval/datasets/arithmetic |
| DROP | Arithmetic, In-Context Reading Comprehension | 55K adversarial questions which require 1) selecting relevant items from the text and 2) computing on them (sorting/counting/…) to get the correct answer | Task: select and count to provide the correct answer| [Paper](https://aclanthology.org/N19-1246/) | [Data](https://allenai.org/data/drop) | |
| Dyck language_HELM | Symbolic manipulation | 500 D_n words between 52 and 100 characters ("word" made of nested brackets/parenthesis) where the last i characters have been removed. | Task: Predict the unique sequence of closing parentheses. | [Paper](https://arxiv.org/abs/2211.09110) | [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/dyck_language_scenario.py) <br>Also has a different version in BigBench | |
| HellaSwag | Cloze/Completion | 60K adversarially filtered multiple choice Q/A | Choose the correct next sentence (from captions or WikiHow) | [Paper](https://aclanthology.org/P19-1472/) | [Github](https://github.com/rowanz/hellaswag/tree/master/data) | |
| HumanEval | Code task, Text-to-code | 164 hand written programming problems with function signature, docstring, body + unit tests | Aim is to complete function to pass unit tests | [Paper](https://arxiv.org/abs/2107.03374) | [Hugging Face](https://huggingface.co/datasets/openai_humaneval) | |
| IMDB | Sentiment Analysis | 50K reviews from IMDB, with even positive (score ≥ 7) /negative (score ≤ 4) reviews (no neutral). | Classify positive/negative review. | [Paper](https://aclanthology.org/P11-1015/) | [Website](https://ai.stanford.edu/~amaas/data/sentiment/) | |
| LAMBADA | Cloze/Completion | 10K Narrative contexts (from the BookCorpus) followed by a sentence where the last word is masked and must be predicted. Specifically built to force use of the context. | Predict the last word. | [Paper](https://aclanthology.org/P16-1144/) | [Zenodo](https://zenodo.org/record/2630551#.YFJVaWT7S_w) | |
| Language Modeling Evaluation_HELM | Language Modeling | Compilation in HELM of several datasets: WikiText-103, ThePile (particularly arXiv, BooksCorpus2, Enron Emails, PubMed Central, Wikipedia), TwitterAAE, ICE. | Task: get conditional log probability of the full sequence (perplexity measure) | [Paper](https://arxiv.org/abs/2211.09110) | [The pile website](https://pile.eleuther.ai/ )<br>[BLIMP Github](https://github.com/alexwarstadt/blimp )<br>[Wikitext data ](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip )<br>[Twitter AAE data](http://slanglab.cs.umass.edu/TwitterAAE/ )<br>[ICE data](https://www.ice-corpora.uzh.ch/en/access.htm) | |
| LegalSupport | Entailment, Real life task, Reasoning | 20K legal entailment scenarios, constructed from state/federal legal opinions (assertion is used as context, and 2 supporting sources ("see X, rule") are selected at random). | Task: finding which rule best supports assertion. | [Paper](https://arxiv.org/abs/2211.09110) | [Data](https://docs.google.com/uc?export=download&id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77) | |
| LinuxKernel_HELM| Generation, Memorisation | 2K randomly sampled functions from the Linux Kernel. | Task: from a random number of lines beginning a function, the model must generate a follow up - measure exact and near-exact reproduction. | [Paper](https://arxiv.org/abs/2211.09110) | [Data](https://drive.google.com/file/d/1Y5piYwil7T6n8toT_-d7NWqVZHh9NVxJ/view) | |
| LSAT | Analytical Reasoning, In-Context Reading Comprehension, Logical Reasoning | 10K questions from the Law School Admission Test (analytical, logical reasoning, and reading comprehension), with context. | Answer the MCQA correctly | [Paper](https://arxiv.org/pdf/2108.00648.pdf) | [Github](https://github.com/zhongwanjun/AR-LSAT/tree/main/data) | |
| Magellan Benchmark | Real life task, Reasoning, Structured data | 23 datasets from several sources containing entities with attributes. Dirty datasets contain deliberate errors, such as attributes being in the wrong column, misspellings, etc. | Task: given two entities from two different tables, determine if they are the same or not. | [Paper](https://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf) | [Github](https://github.com/anhaidgroup/deepmatcher/blob/master/Datasets.md) | It's likely that Abt-Buy and Buy are the same dataset |
| MBPP | Code task, Text-to-code | 1K entry-level Python crowd-sourced programming problems (description, solution, 3 unit test cases) - (58% mathematical, 43% list processing, 19% string processing, 9% integer sequences, and 2% other). | Solve the Python program | [Paper](https://arxiv.org/abs/2108.07732) | [Github](https://github.com/google-research/google-research/tree/master/mbpp )<br>[Hugging Face](https://huggingface.co/datasets/mbpp) | Also contains an edited version (400 items) with unambiguous prompts and good signatures (can be interesting to look at later to see the impact of prompts on code gen) + a MathQA-Python dataset (adaptation of the MathQA dataset) |
| MMLU | Language Understanding | 15K multi-choice Q/A manually collected from various online sources, on many topics (legal, philosophy, economics, psychology, STEM, medicine, etc, - at high school to professional level) | Answer the MCQA | [Paper](https://arxiv.org/abs/2009.03300) | [Hugging Face](https://huggingface.co/datasets/lukaemon/mmlu )<br>[Github](https://github.com/hendrycks/test ) | Seems like a strong/high-quality baseline |
| MRF (Misinfo Reaction Frames) | Generation, Misinformation capabilities | 200K pairs of claims from news headlines (climate change, covid 19, cancer illness, detailed sources in comments) + label (real/misinformation), the former annotated on veracity, likelihood of spread, writer intent by MTurk workers. | Task: must either predict the gold label or generate likely writer intent/reader perception/… | [Paper](https://aclanthology.org/2022.acl-long.222/) | [Github](https://github.com/skgabriel/mrf-modeling) | (Contains data from NELA-GT-2018-2020, SciDCC, Climate-FEVER, CoAID, CoronaVirusFacts/DatosCoronaVirusAlliance Database, ESOC Covid-19 Misinformation Dataset, DETERRENT) |
| MS MARCO | Question Answering, Retrieval | 1M anonymised questions with free-form human generated answers (from relevant web document extracts), some with added rewriting. <br> | Original paper contains 3 tasks: 1) generating the correct answer, if possible, 2) same but answer should make sense even without context, 3) ranking 1000 passages on how relevant they are for the question. | [Paper](https://arxiv.org/abs/1611.09268) | [Github](https://microsoft.github.io/msmarco/) | Contains extended descriptions of QA datasets in lit review.<br>In HELM, only the ranking task is looked at, and relevance is estimated looking at the log-likelihood of the prediction when asking "Does the passage answer the query?" |
| MS MARCO TREC, aka TREC 2019 | Retrieval | Datasets derived from MS MARCO, edited for either passage or document retrieval tasks, either doing full retrieval or top-n reranking (100 for documents, 1000 for passages). (see MS MARCO) | | [Paper](https://arxiv.org/abs/2003.07820) | [Data](https://trec.nist.gov/data/deep2019.html) <br>[Github](https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019.html) | |
| MultiRC | Language Understanding, Question Answering | 6K multiple choice question over a diversity of topics | | [Paper](https://aclanthology.org/N18-1023.pdf) | [Data](https://super.gluebenchmark.com/tasks) | |
| NarrativeQA | Question Answering, Retrieval | 47K free-form human generated questions and answers, linked to 1.5K books (Gutemberg project) and movie scripts (scraped) matched with plot summaries. <br> | Task: from the summary or the story, answer or select the correct answer. | [Paper](https://arxiv.org/abs/1712.07040) | [Github](https://github.com/deepmind/narrativeqa) | For long range context testing, we could use this dataset to do QA from the full stories. Could be interesting for anything conversational imo. |
| Natural Questions | Open domain/Closed book, Question Answering| 207K aggregated google search queries + annotated wikipedia sample answer | | [Paper](https://aclanthology.org/Q19-1026/) | [Data](https://ai.google.com/research/NaturalQuestions/download) | |
| NewsQA | Question Answering | 100K human generated QA pairs from 12K news articles (CNN). Questions were generated from title + summary, answers from question + article, then kept through a validation mechanism. <br>Likely intersects with CNN/DailyMail, as data extraction script was the same. | | [Paper](https://aclanthology.org/W17-2623/) | [Github](https://github.com/Maluuba/newsqa) | |
| OpenBookQA | Common Sense, Reasoning | 6K sentences, science reasoning needing common sense knowledge to extrapolate to new situations | | [Paper](https://arxiv.org/abs/1809.02789) | [Data](https://allenai.org/data/open-book-qa) | |
| PIQA | Common Sense, Reasoning | 20K physical common sense reasoning situations, | select the correct action to do from a context and answers | [Paper](https://arxiv.org/abs/1911.11641) | [Data](https://yonatanbisk.com/piqa/data/) | |
| PopularBooksCorpus_HELM | Generation, Memorisation | 20 books from BooksCorpus which appear in a list of bestsellers. | Task: from a random number of tokens beginning the first paragraph of the book, the model must generate a follow up - measure exact and near-exact reproduction. | [Paper](https://arxiv.org/abs/2211.09110) | [Data](https://drive.google.com/file/d/1RT29rRKNNXKgZBhXNbqevLwR440g44it/view) | |
| QuAC | In-Context Reading Comprehension | 100K questions in information seeking QA contexts (used Wikipedia to generate dataset) | | [Paper](https://aclanthology.org/D18-1241/) | [Data](https://quac.ai/) | |
| RACE | In-Context Reading Comprehension | 100K questions from English reading comprehension exam for Chinese mid/high school students | | [Paper](https://aclanthology.org/D17-1082/) | [Data](https://www.cs.cmu.edu/~glai1/data/race/) | |
| RAFT | Real life task, Text classification | Compilation of 11 datasets of naturally occurring classification tasks, of between 150 and 5K test items. | Task: in few shot from 50 labeled examples, provide meaningful labels. (Domains: medical, finance, research, english language, law, physics, AI safety, social networks) | [Paper](https://arxiv.org/abs/2109.14076) | [Hugging Face](https://huggingface.co/datasets/ought/raft) | Corpus: (ADE Corpus v2, Banking77, NeurIPS 2020 impact statement risks, OneStopEnglish, Overrruling, Systematic review inclusion, TAI safety research, Terms of Service, TweetEval Hate, Twitter complaints, + Semiconductor org types, created for this) |
| RealToxicityPrompts | Generation, Toxicity detection | 100K natural occurring sentences (selected from OpenWebText corpus, basically = reddit, and scored for toxicity with the PerspectiveAPI) split in two to create a prompt and continuation. | Task: generate the continuation from the sentence start, then toxicity evaluated with PerspectiveAPI. | [Paper](https://arxiv.org/abs/2009.11462) | [Data](https://allenai.org/data/real-toxicity-prompts)<br>[Github](https://github.com/allenai/real-toxicity-prompts) (the repo lacks a lot of info) | |
| ReCoRD | Language Understanding | 120K passage/cloze query/answer examples from news (CNN, DailyMail) with human filtering | | [Paper](https://arxiv.org/abs/1810.12885) | [Data](https://super.gluebenchmark.com/tasks) | |
| RTE | Language Understanding | 3K compilation of competition data on entailement | | [Paper](https://w4ngatang.github.io/static/papers/superglue.pdf) | [Data](https://super.gluebenchmark.com/tasks) | |
| SAT analogies | Language Understanding | 374 SAT analogy problem prior to 2005 (a is to b what c is to multiple choice questions; words are not the most frequent) | | [Paper](https://arxiv.org/pdf/2005.14165.pdf) | [Data dev](https://goo.gl/XWjas1) <br>[Data test](https://goo.gl/BcTtB4) | |
| SIQA | Question Answering | | || | |
| SQuADv2 | In-Context Reading Comprehension | Combines SQuAD with 50K unanswerable questions | from a context, give an answer, but only if possible| [Paper](https://arxiv.org/abs/1806.03822) | [Github](https://rajpurkar.github.io/SQuAD-explorer/) | |
| StoryCloze | Cloze/Completion, Common Sense | 50K 5-sentences commonsense stories | choose the correct ending | [Paper](https://aclanthology.org/N16-1098/) | [Hugging Face](https://huggingface.co/datasets/story_cloze) | |
| StrategyQA | Common Sense, Reasoning | 2.8K questions needing reasoning from implicit knowledge | | [Paper](https://arxiv.org/abs/2101.02235) | | Best results with an external calculator added |
| Synthetic reasoning (natural) | Logical Reasoning, Reasoning | Synthetic data generated on the fly, containing a set of synthetic rules (conditional statements), facts (attributes), and the logical gold output. | | [Paper](https://arxiv.org/abs/2211.09110) | Can be generated with [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_natural_scenario.py) | Also called rule_induct in HELM |
| Synthetic reasoning (symbolic)_HELM | Logical Reasoning, Symbolic manipulation | Synthetic data generated on the fly using a pattern template. | Either test if the model is able to identify patterns ("beach + beach - pear" has "A + A - B" as pattern) or if the model can substitute strings in a given pattern. | [Paper](https://arxiv.org/abs/2211.09110) | Can be generated with [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_scenario.py) | |
| TriviaQA | Open domain/Closed book, Question Answering| 95K trivia QA (compositional questions, syntactic variability) | | [Paper](https://aclanthology.org/P17-1147/) | [Data](https://nlp.cs.washington.edu/triviaqa/) | |
| TruthfulQA | Question Answering | 817 questions about tricky factual claims (common misconceptions, falsehoods, …) over 38 categories, with true and false reference answers + a source to support true answers (+ 380 added questions). | | [Paper](https://arxiv.org/abs/2109.07958) | [Github](https://github.com/sylinrl/TruthfulQA) | |
| TyDiQA-GoldP | Multilingual, Question Answering | 204K multilingual QA pairs (unconstrained question elicitation from prompts, then Wikipedia article retrieval, and specific answer selection in the article if possible) (en, ar, ben, fin, ind, ja, ko, ru, tel, th and kiswahili). | MCQA | [Paper](https://aclanthology.org/2020.tacl-1.30/) | [Github](https://github.com/google-research-datasets/tydiqa) | Dataset generated can present interesting underspecification of questions and mismatch between question and answers language level. Might be harder than other datasets |
| Web Questions | Open domain/Closed book, Question Answering| Extracted 100K "Wh?" questions from Google Search API, then annotated by MTurkers - I suspect answers are partially out of date | MCQA | [Paper](https://aclanthology.org/D13-1160/) | [Website](https://nlp.stanford.edu/software/sempre/)| |
| WebNLG | Generation, Verbalization | 13K mappings between triples (subject, property, object, constructed from DBPedia, which is a KB from Wikipedia) and sentence verbalization (by crowd workers), about specific topics (astronauts, universities, monuments, buildings, characters from comics, food, airports, sports teams, written works). | Task: verbalize in a grammatical way. | [Paper](https://aclanthology.org/P17-1017.pdf) | [Hugging Face](https://huggingface.co/datasets/web_nlg) | <br>There was a sentence selection for fluency and the sentences generated are relatively simple, but there is no description of annotators/crowdsourcers origins > maybe some data is not in "standard English". |
| WiC | Language Understanding | 7K, classification of whether a word occurring in two different contexts has the same meaning or not | | [Paper](https://aclanthology.org/N19-1128/) | [Site](https://super.gluebenchmark.com/tasks) | |
| WikiFact_HELM | Cloze/Completion | 12 domains with 1K triples (subject, relation, object) sampled from Wikipedia and cleaned. | Task: predict missing item in the sentence made of the relation. | [Paper](https://arxiv.org/abs/2211.09110) | [Codalab](https://worksheets.codalab.org/rest/bundles/0x8c3b60eb7c6b462e822a150f194d3b35/)<br>[Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/wikifact_scenario.py) | |
| WikiLingua | Generation, Multilingual, Summarization | 43K article/summary pairs constructed from WikiHow in 18 languages (on the site, articles are written with a summary sentence + detailed paragraph per step: in the dataset, summaries are the concatenation of the summary sentences, and articles of the detailed paragraphs| Summarization | [Paper](https://aclanthology.org/2020.findings-emnlp.360/) | [Github](https://github.com/esdurmus/Wikilingua ) | Palm: Prefixed with a prompt, truncated article to 2048 tokens<br>I suspect data creation can leads to very "robotic" language for the summary baseline, which could underscore more fluid summaries - though ROUGE shouldn't be too prone to that). |
| Winogender | Bias detection | | || | |
| Winograd | Reasoning, Winograd | 273 to 285 examples where one must disambiguate who/what a pronoun is referring to on sentence specially constructed to be ambiguous to statistics not to humans | Disambiguation of pronoun | [Paper](https://dl.acm.org/doi/10.5555/3031843.3031909) | [Website](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WSCollection.xml) | Not sure if GPT3 was evaled on this one or the SuperGLUE one |
| WinoGrande | Reasoning, Winograd | 43K sentences Adversarial Winograd | | [Paper](https://arxiv.org/abs/1907.10641) | [Website](https://winogrande.allenai.org/) | |
| WSC | Language Understanding, Winograd | WinoGrad Schema Challenge (see Winograd) | | [Paper](https://w4ngatang.github.io/static/papers/superglue.pdf) | [Website](https://super.gluebenchmark.com/tasks) | |
| XSUM | Summarization | 226K news articles (BBC, 2010 to 2017) matched with their single sentence summary (comes from the article). Task: Summarize. (Domains: News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts) | | [Paper](https://aclanthology.org/D18-1206/) | [Github](https://github.com/EdinburghNLP/XSum) | |
| XSum | Generation, Summarization | 226K news summary/article pairs from the BBC (2010 - 2017) extracted from the WayBack machine | | [Paper](https://aclanthology.org/D18-1206/) | [Hugging Face](https://huggingface.co/datasets/xsum)| Could be interesting to manually check if the model recent knowledge creates discrepancies in the summaries of old news. |

## Dataset ideas to manually reproduce

| Evaluation name                | Task type                                                  | Task content                                                                                                                                                                                                                                                              | Source                                                                             | Dataset                                                                                  | Comments                                                                                                                                                                                         |     |
| ------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --- |
| ✍️ GSM8K-Python                | Code task, Text-to-code                                    | Python version of the GSM8K dataset (8.5K grade school math problems)                                                                                                                                                                                                     | [Paper](https://arxiv.org/abs/2204.02311)                                          | N/A                                                                                      |                                                                                                                                                                                                  |     |
| ✍️ MRF                         | Generation, Manual evaluation, Misinformation capabilities | 250 headlines extracted from the MRF dataset, grouped in 80 clusters by thesis. Task: from the thesis + 5 headlines, the model must generate plausible headlines which supports the thesis. Annotators evaluate if 1) the headline supports the thesis and 2) looks real. | [Paper](https://arxiv.org/abs/2211.09110)                                          | [Data](https://drive.google.com/uc?export=download&id=1uVJbsgPCHFAvH43I6SVvU3Ayo8dh-y_N) | See [report](https://cset.georgetown.edu/publication/truth-lies-and-automation/) page 6 for a detailed explanation of the original process, plus sections 8.5.2, E.5, and 5.5 in the HELM paper. |     |
| ✍️ News article generation     | Generation                                                 | Generated 25 articles from titles and subtitles, 80 humans had to classify if generated or original                                                                                                                                                                       | [Paper](https://arxiv.org/abs/2005.14165)                                          |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️  Numeracy Prediction        | Symbolic manipulation                                      | "requires the model to perform symbolic regression given a few examples, and apply the number relationship to a new input"                                                                                                                                                |                                                                                    | [Paper](https://arxiv.org/abs/2211.09110)                                                | [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/numeracy_scenario.py)                                                                                      |     |
| ✍️ SVG datasets                |                                                            | Could construct an SVG dataset to see if models can indeed generate or interpret SVG drawings                                                                                                                                                                            | [Twitter thread](https://twitter.com/zswitten/status/1631178997508997120)          |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️ Theory of the mind datasets |                                                            | Could likely be easy to generate                                                                                                                                                                                                                                          | [Paper](https://arxiv.org/abs/2302.08399)                                          |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️ Wedging prompts             | Generation, Manual evaluation, Misinformation capabilities | 11 prompts with specific intent (ex: influence voting behaviors, target specific groups by generate pro/anti X rhetoric) augmented with 3 examples. Task: generate follow up examples.  <br>                                                                              | [Paper](https://cset.georgetown.edu/wp-content/uploads/CSET-Truth-Lies-and-Automation.pdf) | [Data](https://drive.google.com/uc?export=download&id=1kWB3_F4Tobc_oVGC_T-a5DHEh-AB4GTc) | In HELM: use manual evaluation to determine if the message generate 1) addresses targeted group; 2) supports desired message; 3) is divisive                                                     |     |
| ✍️ Word scrambling             | Symbolic manipulation                                      | 10K examples for 5 tasks of 5 character manipulation tasks (word with cycled letters, anagrammed, random insertions, reversed). Model needs to recover the original word                                                                                                  | [Paper](https://arxiv.org/abs/2005.14165)                                          |                                                                                          | Easy to generate/automate, see Section 3.9.2 of GPT3 paper                                                                                                                                       |     |
