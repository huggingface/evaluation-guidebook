# 자동 평가 설계하기

## 데이터셋 선택하기

평가를 위해 기존 데이터셋을 선택하거나([일부 평가 데이터셋](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/automated-benchmarks/some-evaluation-datasets.md) 예시 참조) 직접 설계할 수 있습니다. 이 과정에서 **평가 결과는 평가 데이터셋의 품질에 따라 달라진다**는 점을 명심하는 것이 매우 중요합니다.

### 기존 데이터셋 선택하기

반드시 데이터셋의 구성 요소를 살펴봐야 합니다.

#### 생성 과정

- **실제 샘플을 누가 만들었나요?**
  개인적으로, 전문가가 만든 데이터셋 > 돈을 주고 고용한 주석자자(annotator) 데이터셋 ~ 크라우드소싱 데이터셋 > MTurk 데이터셋 순으로 품질이 좋습니다.
  또한 데이터 카드를 확인하여 주석자 인구통계 정보를 찾아보세요 - 이는 데이터셋의 언어 다양성을 이해하는 데 중요할 수 있습니다.
- **모든 샘플이 다른 주석자나 저자에 의해 검토되었나요?**
  다음 사항을 알아야 합니다:

  - 샘플에 대한 주석자자 간 점수가 높은지(= 주석자들이 서로 동의하는지?)
  - 그리고/또는 전체 데이터셋이 저자에 의해 검토되었는지.
    이는 특히 저임금 주석자(AWS Mechanical Turk 같은)의 도움을 받은 데이터셋에서 중요합니다. 이들은 보통 타겟겟 언어의 원어민이 아니기 때문에 오타/문법 오류/이해할 수 없는 답변이 있을 수 있습니다.
- **주석자들에게 명확한 데이터 생성 가이드라인이 제공되었나요?**
  다시 말해, 데이터셋이 일관성이 있나요?

#### 샘플

무작위로 50개의 샘플을 선택하여 수동으로 검사하세요:

- *품질 측면*:
  - 프롬프트가 명확하고 모호하지 않은가요?
  - 답변이 정확한가요? (*예: TriviaQA는 질문당 여러 개의 정답(aliases 필드)을 포함하며, 때로는 서로 충돌합니다.*)
  - 정보가 누락되었나요? (*예: MMLU는 여러 질문에서 참조 도식이 누락되어 있습니다.*)
- *작업 관련성*:
  - 이러한 질문들이 LLM을 평가하고자 하는 종류의 질문인가요?
  - 이러한 예시들이 사용 사례와 관련이 있나요?

또한 통계적으로 유의미한 결과를 얻기 위해 데이터셋에 얼마나 많은 샘플이 있는지 확인해야 합니다(자동 벤치마크의 경우 일반적으로 최소 100개의 샘플이 필요합니다).

### 직접 데이터셋 설계하기

자체 데이터셋을 설계할 때 3가지 방법이 있습니다.

#### 기존 데이터 집계하기

다양한 출처에서 기존 데이터를 집계하여 작업과 관련된 능력을 평가할 수 있습니다. 예를 들어, 많은 평가 데이터셋은 인간 평가 데이터셋(MATH, LSAT 등)을 집계하여 구성됩니다. 이 경우 위의 단계를 따르세요.

#### 인간 주석자 활용하기

`인간 평가` 섹션에 인간 주석자 활용에 대한 전체 섹션이 있습니다. [인간 주석자 활용하기](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/human-evaluation/using-human-annotators.md)를 참조하세요.

#### 합성 데이터 사용하기

- **LLM 사용하기**
  이에 대해서는 HF 동료들이 작성한 매우 멋진 [Cosmopedia](https://huggingface.co/blog/cosmopedia) 블로그를 확인해보세요! 주로 합성 훈련 데이터셋을 만드는 방법을 연구하지만, 비슷한 기술을 평가에도 적용할 수 있습니다.
  나중에 데이터셋을 수동으로 확인/필터링/검사하는 것을 잊지 마세요(위의 단계를 따라).
- **규칙 기반 기술 사용하기**
  작업이 허용한다면, 이는 사실상 무한한 샘플을 얻고 오염을 피하는 매우 좋은 방법입니다!
  예시로 [NPHardEval](https://arxiv.org/abs/2312.14890), [DyVal](https://arxiv.org/abs/2309.17167), [MuSR](https://arxiv.org/abs/2310.16049), [BabiQA](https://arxiv.org/abs/1502.05698) 등을 살펴볼 수 있습니다.

## 추론 방법 선택하기

어떤 종류의 추론 방법이 필요한지 선택해야 합니다.

로그 확률(MCQA, 객관식 질문 답변)은 객관식 질문 답변(일반적으로 모델 지식이나 모호성 해소 능력을 테스트하기 위함)에 매우 적합합니다.

- 장점:
  - 모든 모델이 정답에 접근할 수 있도록 보장
  - 모델 "확신도confidence"(및 보정)에 대한 프록시 제공
  - 특히 모델이 하나의 토큰만 예측하도록 요청할 때(A/B/C/D 선택지 인덱스 또는 예/아니오 등) 빠르게 평가 가능
  - 작은 모델의 작업 성능에 대한 신호를 얻을 수 있음
- 단점:
  - 자유롭게 생성할 경우 사용 가능한 선택지 범위를 벗어난 것을 생성했을 작은 모델의 점수를 약간 과대평가함
  - 일부 모델은 [제시된 순서에 따라 특정 선택지를 선호](https://arxiv.org/abs/2309.03882)하여 대표성 없는 평가로 이어질 수 있음

생성(QA, 질문 답변)은 유창성, 추론 또는 모델이 실제로 질문에 답변하는 능력을 테스트하려는 모든 작업에 매우 적합합니다.

- 장점:
  - LLM이 유창한 텍스트를 생성하는 능력과 실제로 상관관계가 있으며, 대부분의 경우 사람들이 실제로 관심을 갖는 부분임
- 단점:
  - 점수를 매기기 더 어려울 수 있음(아래 `메트릭` 섹션 참조)
  - 일반적으로 로그 가능도 평가보다 약간 더 비용이 많이 들며, 특히 샘플링을 포함하는 경우 더욱 그러함

## 프롬프트 선택하기

프롬프트는 다음을 정의합니다:

- 작업에 대해 모델에 제공되는 정보의 양
- 이 정보가 모델에 제시되는 방식

일반적인 MCQA 또는 QA 프롬프트는 일반적으로 다음과 같은 요소로 구성됩니다:

- 작업 프롬프트(선택 사항): 작업을 소개합니다.
- 컨텍스트: 질문에 대한 추가 컨텍스트를 제공합니다.
  - *예: 요약 또는 정보 추출 작업의 경우, 콘텐츠 소스를 제공할 수 있습니다*
- 질문: 프롬프트의 실제 핵심입니다.
- 객관식 평가의 경우, 선택지를 추가할 수 있습니다
- 연결 단어(`질문`, `컨텍스트`, `선택지` 등)

프롬프트를 정의할 때 다음 사항을 알아야 합니다:

- 의미적으로 동등한 프롬프트의 작은 변화도 결과를 크게 변화시킬 수 있으며([문제 해결 재현성](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/troubleshooting/troubleshooting-reproducibility.md)의 `다른 프롬프트` 섹션 참조), 프롬프트 형식은 특정 모델에 유리하거나 불리할 수 있습니다
  - 완화 방법:
    - 비용이 많이 드는 방법은 프롬프트 변형으로 평가를 여러 번 다시 실행하는 것입니다
    - 비용이 적게 드는 방법은 동등한 난이도의 다른 샘플에 할당된 다양한 프롬프트 형식을 사용하여 평가를 한 번 실행하는 것입니다
- 예상되는 형식을 따르도록 모델에 예시를 제공할 수 있으며(few-shot 예시 사용), 연결 단어를 추가하면 전반적으로 도움이 됩니다
- 그러나 모델은 이제 특정 프롬프트 형식에 과적합되는 경향이 있습니다.
  - [이 논문](https://arxiv.org/abs/2407.07890)은 이 주제에 대해 훌륭하며, 특히 일부 모델이 테스트 세트 **형식**에 과적합되어 과대평가될 수 있음을 보여줍니다
  - Open LLM Leaderboard 2에서는 특히 Llama 3.2와 Qwen 2.5가 이러한 이유로 few-shot 설정에서 제공된 프롬프트 형식을 더 이상 따르지 않는 것을 관찰했습니다.
- 여러 메트릭의 경우, 매우 제한된 생성 또는 출력이 필요합니다.
  *[모델 추론 및 평가](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md) 페이지의 `모델 출력 제한` 섹션에서 자세히 알아볼 수 있습니다.*

## 메트릭 선택하기

**로그 확률**을 살펴보는 경우, 메트릭은 간단합니다: 정확도(가장 가능성이 높은 선택이 최선의 선택인 빈도)를 살펴보고 싶을 것입니다. 길이(문자, 토큰 또는 pmi)로 정규화하는 것이 중요합니다. 또한 혼란도(perplexity), 재현율(recall) 또는 F1 점수를 살펴볼 수도 있습니다.

**생성적** 평가의 경우, 메트릭 범위가 더 넓습니다.
다음을 결정해야 합니다:

1. 생성된 결과를 있는 그대로 비교할지, 아니면 먼저 무언가로 정규화할지 결정합니다.
   - 정규화는 잘 설계되지 않으면 쉽게 [불공정할 수 있지만](https://huggingface.co/blog/open-llm-leaderboard-drop), 전반적으로 작업 수준에서 신호를 제공합니다.
   - 이는 수학 평가와 같은 특정 작업에서 매우 중요하며, 형식화된 출력에서 결과를 추출하고 싶을 수 있습니다.
   - 또한 사고 연쇄(Chain of Thought)와 같은 정확도를 위한 추가 메커니즘으로 평가하려는 경우에도 중요하며, 실제 결과에서 추론 과정을 제거해야 합니다.
2. 생성된 결과를 참조와 어떻게 비교할지 결정합니다.
   일치 기반 메트릭(정확한 일치, 접두사 일치 등)부터 요약 및 번역 메트릭(ROUGE, BLEU, 문자 n-gram 비교)까지 다양한 방법을 사용할 수 있습니다. 기존 메트릭 목록은 [여기](https://github.com/huggingface/lighteval/wiki/Metric-List)에서 확인할 수 있으며, 나중에 어떤 상황에서 어떤 메트릭을 사용해야 하는지에 대한 섹션을 추가할 예정입니다.

더 일반적으로, 메트릭을 선택할 때 작업이 실제로 무엇에 관한 것인지 염두에 두어야 합니다. 일부 도메인(예: 의료, 공개 상호작용이 있는 챗봇)의 경우, 평균 성능을 측정하는 것이 아니라 **최악의 성능**(출력의 의료 품질, 유해성 등)을 평가하는 방법이 필요합니다. (*더 자세히 알아보려면 이 [블로그](https://ehudreiter.com/2024/07/10/challenges-in-evaluating-llms/)를 참조하세요*)

## 스마트한 새로운 작업: 기능 테스트는 어떨까요?

코드 분야에서는 생성된 프로그램을 의미론적으로만 평가하는 것이 아니라 실제 기능도 평가하고 싶습니다. 따라서 프롬프트에 따라 생성된 코드가 작업에 맞게 설계된 단위 테스트 모음을 올바르게 통과하는지 확인하는 것이 좋은 방법입니다.

이 기능적 접근 방식은 매우 유망합니다:

- 테스트 케이스를 더 쉽게 생성할 수 있습니다(많은 경우 규칙 기반 테스트 케이스를 생성할 수 있음)
- 따라서 과적합을 줄입니다
- 특정 활성 능력에 대해 모델을 테스트합니다

그러나 이는 텍스트로 변환하기 위해 창의성이 필요한 접근 방식입니다!

좋은 예로 IFEval이 있는데, 이는 모델이 지시를 따를 수 있는지 테스트하는 평가 벤치마크입니다. 이는 여러 형식 지시사항(*이 개수의 글머리 기호 추가하기. 한 문장만 대문자로 쓰기.* 등)을 만들고 형식이 엄격하게 준수되는지 테스트하는 방식으로 작동합니다. 이 아이디어를 분석할 텍스트의 다른 기능으로 확장하기 위해서는 분명히 더 많은 작업이 필요합니다!
