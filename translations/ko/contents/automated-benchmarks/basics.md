# 기초

*참고: 이 내용 중 일부는 [평가에 관한 블로그](https://huggingface.co/blog/clefourrier/llm-evaluation)와 중복됩니다*
## 자동화된 벤치마크란 무엇인가?

자동화된 벤치마크는 일반적으로 다음과 같은 방식으로 작동합니다: 모델이 특정 작업에서 얼마나 잘 수행하는지 알고 싶을 것입니다. 이 작업은 `내 모델이 스팸 이메일과 일반 이메일을 얼마나 잘 분류할 수 있는가?`와 같은 잘 정의된 구체적인 **작업**일 수도 있고, `내 모델이 수학에 얼마나 능숙한가?`와 같은 더 추상적이고 일반적인 **능력**일 수도 있습니다.

이를 바탕으로 다음을 사용하여 평가를 구성합니다:
- **데이터셋**은 **샘플**로 구성됩니다.
	- 이 샘플들은 모델에 대한 입력을 포함하며, 때로는 모델의 출력과 비교할 참조(골드라고 함)와 결합됩니다.
	- 샘플은 일반적으로 테스트하려는 것을 모방하도록 설계됩니다. 예를 들어, 이메일 분류를 살펴보는 경우 스팸 및 비스팸 이메일의 데이터셋을 만들고 어려운 경계 사례를 포함하려고 시도합니다.
- **메트릭**.
	- 메트릭은 모델을 평가하는 방법입니다.
	  예: 모델이 스팸을 얼마나 정확하게 분류할 수 있는지(잘 분류된 샘플 점수 = 1, 잘못 분류된 샘플 = 0).
	- 메트릭은 이 점수를 매기기 위해 모델의 출력을 사용합니다. LLM의 경우, 사람들은 주로 두 가지 종류의 출력을 고려합니다:
		- 입력에 따라 모델이 생성한 텍스트(*생성적 평가*)
		- 모델에 제공된 하나 또는 여러 시퀀스의 로그 확률(*객관식 평가*, 때로는 MCQA라고 하거나 *혼란도Perplexity 평가*)
		- 이에 대한 자세한 정보는 [모델 추론 및 평가](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md) 페이지를 확인해야 합니다.

이는 모델이 이전에 노출된 적이 없는 데이터(모델 훈련 세트에 없는 데이터)에서 수행하는 것이 더 흥미롭습니다. 왜냐하면 모델이 잘 **일반화**하는지 테스트하고 싶기 때문입니다. 예를 들어, 가짜 은행에 관한 스팸 이메일만 본 후에 '건강' 제품에 관한 스팸 이메일을 분류할 수 있는지 확인하는 것입니다.

참고: *훈련 데이터에서만 잘 예측할 수 있고 더 높은 수준의 일반적인 패턴을 잠재적으로 학습하지 못한 모델은 **과적합**되었다고 합니다. 주제를 이해하지 않고 시험 문제를 암기한 학생과 유사하게, 이미 훈련 세트에 있던 데이터로 LLM을 평가하는 것은 그들이 가지고 있지 않은 능력에 대해 점수를 매기는 것입니다.*

## 자동화된 벤치마크 사용의 장단점
자동화된 벤치마크는 다음과 같은 장점이 있습니다:
- **일관성 및 재현성**: 동일한 모델에서 동일한 자동화된 벤치마크를 10번 실행하면 동일한 결과를 얻을 수 있습니다(하드웨어 변동이나 모델의 내재적 무작위성 제외). 이는 특정 작업에 대한 모델의 공정한 순위를 쉽게 만들 수 있음을 의미합니다.
- **제한된 비용으로 규모 확장**: 현재 모델을 평가하는 가장 저렴한 방법 중 하나입니다.
- **이해 가능성**: 대부분의 자동화된 메트릭은 매우 이해하기 쉽습니다.
  *예: 정확한 일치는 생성된 텍스트가 참조와 완벽하게 일치하는지 알려주며, 정확도 점수는 선택된 선택지가 올바른 경우가 얼마나 많은지 알려줍니다(이는 `BLEU` 또는 `ROUGE`와 같은 메트릭의 경우 약간 덜 그럴 수 있습니다).*
- **데이터셋 품질**: 많은 자동화된 벤치마크는 전문가가 생성한 데이터셋이나 기존의 고품질 데이터(MMLU 또는 MATH와 같은)를 사용합니다. 그러나 이러한 데이터셋이 완벽하다는 의미는 아닙니다: MMLU의 경우, 구문 분석 문제부터 실제로 의미 없는 질문까지 샘플에서 여러 오류가 나중에 식별되어 MMLU-Pro 및 MMLU-Redux와 같은 여러 후속 데이터셋이 생성되었습니다.

그러나 다음과 같은 제한 사항도 있습니다:
- **더 복잡한 작업에서의 제한된 사용**: 자동화된 벤치마크는 성능을 쉽게 정의하고 평가할 수 있는 작업(예: 분류)에서 잘 작동합니다. 반면에 더 복잡한 능력은 잘 정의되고 정확한 작업으로 분해하기가 더 어렵습니다.
  *예: "수학을 잘한다"는 것은 무엇을 의미하나요? 산술을 잘하는 것인가요? - 논리를 잘하는 것인가요? - 새로운 수학적 개념에 대해 추론할 수 있는 것인가요?*
  이로 인해 더 **일반적인** 평가의 사용으로 이어졌으며, 이는 더 이상 능력을 하위 작업으로 분해하지 않고 일반적인 성능이 우리가 측정하고자 하는 것에 대한 **좋은 대리 지표Good Proxy**가 될 것이라고 가정합니다.
- **오염**: 데이터셋이 일반 텍스트로 공개적으로 게시되면 모델 훈련 데이터셋에 포함될 것입니다. 이는 모델을 평가할 때 해당 모델이 이전에 평가 데이터를 분석하지 않았다는 보장이 없음을 의미합니다.