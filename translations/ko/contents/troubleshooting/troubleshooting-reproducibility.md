# 재현성 문제 해결하기

최근 멋진 새 모델에 관한 기술 보고서를 읽고 자신의 기계에서 그 결과를 재현하고 싶지만... 성공하지 못하고 있다고 가정해 봅시다.
그 이유를 살펴보겠습니다.

## 다른 코드 베이스
소수점까지 정확한 평가 점수를 재현하려면, 먼저 재현하려는 논문과 정확히 동일한 코드 베이스를 사용하고 있는지 확인해야 합니다.

일반적으로 이는 저자가 제공한 평가 기본 코드를 사용하거나 Eleuther의 AI `lm_eval` 또는 HuggingFace의 `lighteval`과 같은 표준 라이브러리에서 제공하는 구현을 사용하는 것을 의미합니다. 그러나 평가를 위한 코드 소스가 제공되지 않은 경우, 안타깝게도 결과를 정확하게 재현하기는 어려울 것입니다.

다른 구현을 사용할 때 어떤 종류의 불일치가 발생하는지 쉽게 이해하고 싶다면, HuggingFace의 평가 팀과 함께 작성한 [이 블로그](https://huggingface.co/blog/open-llm-leaderboard-mmlu) (⭐)를 살펴볼 수 있습니다. 이 블로그는 MMLU 평가의 3가지 일반적인 구현(`lm_eval`, `helm`, 원저자 구현)에서 관찰된 차이점과 그것이 모델 점수에 어떤 영향을 미치는지 연구합니다.

*참고: 이러한 이유로 Hugging Face 팀은 [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)를 시작하여 내부 실험과 비교할 수 있도록 모델 점수의 통일되고 균일한 비교를 얻기로 결정했습니다.*

### 구현이 다를 수 있는 다른 미묘한 방법들
동일한 코드 베이스를 사용하더라도 다음과 같은 것들이 쉽게 잘못될 수 있다는 것을 관찰했습니다:
- **다른 랜덤 시드.**
	- 일반적으로 추론은 훈련보다 랜덤 시드의 영향을 덜 받습니다. 그러나 여전히 일부 CUDA 작업에 영향을 미칠 수 있고(PyTorch의 [재현성](https://pytorch.org/docs/stable/notes/randomness.html) 페이지 참조), 탐욕적이지 않은 생성 전략을 사용하는 경우 예측이 변경될 수 있습니다. 또한 퓨샷을 사용하는 경우 프롬프트와 일부 전처리 또는 후처리 함수에도 영향을 줄 수 있습니다.
	  -> 작은 변화가 몇 점의 차이를 가져올 수 있습니다.
- **실제로 다른 메트릭**.
  지표는 동일한 이름을 공유하더라도 실제로는 다를 수 있습니다. 몇 가지 예:
	- 원래 구현이 *로그 가능도* `정확히 일치`(다양한 가능한 답변의 로그 확률 계산)이고 당신이 *생성적* `정확히 일치`(주요 탐욕적 생성과 참조만 비교)를 사용하고 있다면, 동일한 점수를 얻지 못할 것입니다.
	- 우리는 또한 평가 코드 베이스에서 `정확히 일치`로 정의된 많은 작업이 실제로는 `접두사 정확히 일치`(생성의 시작 부분만 참조와 비교), `접미사 정확히 일치`(반대), 또는 `준 정확히 일치`(정규화와 함께 정확히 일치)였음을 보았습니다.
	 -> 따라서 무슨 일이 일어나고 있는지 판단하기 위해 지표 이름에만 의존할 수 없으며, 코드를 살펴봐야 합니다.
- **다른 정규화**.
	- 위의 `정확히 일치` 비교 예로 돌아가면, `lm_eval` v1에서는 많은 작업이 단순히 생성적 `정확히 일치`로 이름이 지정되었습니다: 이로부터 예측이 참조와 *그대로 비교*된다고 가정할 수 있습니다.
	  코드를 살펴보면, 예측은 참조와 비교되기 전에 정규화 단계(구두점 제거, 숫자 동질화 등)를 거칩니다. 이는 분명히 결과를 상당히 많이 변경할 것입니다.
	  (`lm_eval` v2는 이제 대부분의 지표 이름에 정규화 이름을 포함합니다.)
	 -> 이는 특히 수학 평가와 같이 많은 정규화/답변 후처리가 필요한 작업에서 가장 쉽게 잘못될 수 있는 것 중 하나입니다(생성된 설명에서 답변을 추출하려는 경우).

## 다른 프롬프트
프롬프트 변형에 영향을 미치는 3가지 주요 요소가 있습니다.
### 프롬프트 자체
프롬프트 형식은 점수를 크게 변경할 수 있고 변경할 것입니다.

예를 들어, 다중 선택 질문 답변의 경우 선택지를 제시할 때 다음과 같은 매우 간단한 변형이 포함됩니다:
```
Question: <질문 텍스트>
Choices:
```
```markdown
| A. <선택지 A> | (A) <선택지 A> | <선택지 A> | 
| B. <선택지 B> | (B) <선택지 B> | <선택지 B> | 
| C. <선택지 C> | (C) <선택지 C> | <선택지 C> | 
| D. <선택지 D> | (D) <선택지 D> | <선택지 D> | 
```
```
Answer: 
```
그리고 `A`/`B`/`C`/`D` 또는 `<선택지 A/B/C/D>`를 예측합니다.

이러한 프롬프트는 정확히 동일한 내용을 포함하기 때문에 **의미론적으로 동등**합니다 - 그러나 *동일한 모델에 대해 몇 점의 차이*를 가져올 수 있습니다. 우리는 [여기](https://x.com/clefourrier/status/1777319187913875893/photo/1)에서 이에 대한 실험을 했고(동일한 모델에 대해 최대 7점의 차이를 볼 수 있음) [논문에서도 유사한 결과를 관찰](https://arxiv.org/abs/2310.11324)했습니다.

일부 작업은 작업 프롬프트(예: `다음 질문은 <주제>에 관한 것입니다`)로 시작하기도 합니다 - 이의 존재 여부도 점수에 영향을 미칩니다.

또한 [이 훌륭한 논문](https://arxiv.org/abs/2407.07890)⭐은 이의 부작용을 강조합니다: 많은 모델이 이제 평가 시 다른 프롬프트에 적응하는 비용을 들여 벤치마크 프롬프트와 답변 형식에 과적합하도록 훈련되고 있습니다.

이는 Llama3.1 모델에 대한 Open LLM Leaderboard 2에서 관찰한 현상입니다. 그들은 우리의 MATH-Hard 평가에 올바른 답변을 예측했지만, GSM8K 프롬프트와 답변 형식(다른 수학 평가)에 과적합하여 퓨샷에서 제공된 템플릿에 맞출 수 없어 낮은 점수를 받았습니다.
### 시스템 프롬프트와 채팅 템플릿
채팅 모델은 일반적으로 명령/선호도 훈련 또는 미세 조정을 거쳤습니다. 이 단계에서 그들은 추론할 때 특정 템플릿을 따르도록 배웠습니다. 예를 들어, 템플릿은 특정 토큰(일반적으로 `System: `)이 접두된 일반 프롬프트(`시스템 프롬프트`라고 함)로 대화 라운드를 시작해야 할 수 있습니다. 해당 프롬프트는 페르소나의 내용이나 일반적인 답변 스타일 지침과 같은 모델에 대한 높은 수준의 지침을 제공하기 위해 있습니다. 대화 라운드는 또한 쿼리에 대한 `User`와 답변에 대한 `Assistant`와 같은 접두어 키워드를 텍스트에 추가해야 할 수 있습니다.

퓨샷을 사용할 때 예제를 다중 턴(사용자/어시스턴트 턴 모방)으로 제공할지 아니면 한 번에(단일 사용자 프롬프트로) 제공할지도 선택해야 합니다.

추론 시 모델이 예상하는 채팅 템플릿을 따르지 않으면 수렴된 확률 공간 밖으로 출력을 유도하므로 성능이 크게 저하됩니다.

### 퓨샷 샘플
퓨샷 샘플에서는 두 가지가 쉽게 잘못될 수 있습니다(퓨샷이 무엇인지 확실하지 않다면 `general-knowledge/Model inference`를 참조하세요).

당연히, 참조 작업과 **동일한 수의 퓨샷 샘플**을 사용해야 합니다.

그러나 또한 비교하려는 모델과 **정확히 동일한 샘플**을 사용해야 합니다. 다른 샘플을 사용하면 결과가 변경됩니다(일부 샘플이 다른 샘플보다 작업을 더 잘 표현한다고 가정하면 그리 놀랍지 않습니다). 더 놀라운 점은: 정확히 동일한 샘플을 사용할 뿐만 아니라 **정확히 동일한 순서**로 제시해야 합니다. 동일한 샘플에 대해 순서를 변경하면 MMLU의 일부 하위 집합에서 최대 3점의 차이를 관찰했습니다([여기](https://huggingface.co/blog/evaluation-structured-outputs)에서 일부 결과를 볼 수 있으며, 세 번째 컬러그리드입니다).

이 또한 랜덤 시드에 주의를 기울여야 하는 부분입니다.

## 다른 생성 매개변수
생성적 평가의 경우 주의해야 할 매개변수는 다음과 같습니다:
- **동일한 문장 종료 토큰**을 사용하고 있는지 확인
- 모델이 평가를 위해 **동일한 수의 토큰을 생성**할 수 있도록 허용하고 있는지 확인
- 샘플링을 사용하는 경우 **동일한 시드/온도 매개변수**를 사용하고 있는지 확인

## 다른 모델 로딩
우리가 관찰한 차이의 몇 가지 원인은 다음과 같습니다:
- **다른 하드웨어** 사용.
  PyTorch는 하드웨어 간에 비결정적 작업의 재현성을 보장하지 않습니다.
- **다른 라이브러리** 사용.
  예를 들어, 추론을 위해 `transformers` 대 `vllm`을 백엔드로 사용하는 경우, 행렬 계산은 정확히 동일한 방식으로 관리되지 않습니다.
- **다른 배치 크기** 사용.
  여러 평가 라이브러리와 모델 백엔드에서 다른 배치 크기를 사용하면 추론 결과가 변경된다고 문서화되어 있습니다 - 완전히 재현 가능한 평가를 원한다면 배치 크기를 고정해야 하지만, 메모리 문제로 항상 가능하지 않을 수 있습니다.
- 모델 가중치에 대해 **다른 로딩 정밀도** 사용.
  낮은 정밀도를 사용하면 메모리와 추론 비용을 줄일 수 있지만, 가중치의 다른 버전을 사용하므로 수치 결과도 변경됩니다.