# 추론 문제 해결하기

## 내 모델이 매우 느려요!
### 배치 크기 변경하기
절대적인 재현성(특정 하드웨어와 특정 평가 프롬프트 기준)을 원한다면, 아마도 배치 크기 1을 사용하고 있을 것입니다. 그러나 더 큰 배치 크기로 이동하면 평가 속도가 빨라질 가능성이 높습니다(하드웨어의 메모리 요구 사항 내에 맞는 경우).

### 데이터 병렬 처리
단일 GPU에 모델을 로드하는 대신 여러 GPU에 모델을 복제하고, 각 GPU 복사본에 데이터의 하위 집합을 제공한 다음 계산 결과를 집계할 수도 있습니다.
이는 각 데이터 스트림이 다른 스트림과 동시에 병렬로 처리되어 총 실행 시간을 GPU 수로 나누게 된다는 의미입니다.
그러나 가능하다면 노드 간 병목 현상을 피하기 위해 모든 GPU가 단일 노드에 있어야 합니다.

### 추론 코드 변경하기
모든 추론 라이브러리가 동일한 속도로 실행되는 것은 아니며, 일부 코드는 다른 코드보다 더 최적화되어 있습니다. 어떤 라이브러리가 가장 빠른 추론을 제공하는지 찾기 위해 약간의
실험이 필요할 것입니다. PyTorch를 사용하고 있다면 [여기](https://pytorch.org/serve/performance_checklist.html)에서 모델 추론 최적화 체크리스트를 살펴보는 것을 권장합니다.

### 정밀도 변경하기
모델이 매우 느리다면, 계산의 정밀도를 줄여 크기를 줄일 수 있습니다. float32로 저장된 모델은 매우 정밀한 계산(저장된 숫자당 32비트 사용!)을 수행하지만 메모리와 계산 비용이 많이 듭니다. `bfloat16` 또는 `float16`(절반의 정밀도)으로 이동하면 거의 영향을 미치지 않는 정밀도 손실로 모델 속도가 두 배 빨라질 수 있습니다. 속도를 더 높이고 싶다면 `gptq` 또는 `bitsandbytes` 등을 사용하여 8비트나 4비트로 더 양자화할 수 있습니다. n비트 행렬 계산이 더 빠르고 모델이 메모리에서 더 적은 공간을 차지할 것입니다(그러나 일부 양자화 라이브러리는 약간 느릴 수 있으므로 사용 사례에 맞게 테스트해 보세요!).

## 내 모델이 매우 큽니다!
### 메모리 요구 사항 추정하기
주어진 모델을 로드하는 데 필요한 최소 이론적 메모리(따라서 하드웨어)를 **다음 공식**으로 추정할 수 있습니다:

`<메모리 (GB)> = <파라미터 수 (G)> * <정밀도 팩터>`

8비트를 1바이트에 저장할 수 있으므로, 필요한 메모리는 총 파라미터 수에 하나의 파라미터를 저장하는 데 필요한 바이트 수를 곱한 값입니다. 따라서 정밀도 팩터는 `float32`의 경우 4, `float16` 또는 `bfloat16`의 경우 2, `8bit`의 경우 1, `4bit` 모델의 경우 0.5 등입니다.

이게 다입니다!

사실 `<메모리 (GB)> = <파라미터 수 (G)> * (<정밀도 팩터> * 110%)`를 사용하는 것이 좋습니다. 추론은 모델을 로드하는 것보다 약간 더 많은 메모리가 필요하기 때문에(배치도 로드해야 함) 더 안전할 것입니다.

### 모델이 GPU에 맞지 않으면 어떻게 해야 할까요?
#### 양자화
첫 번째 명백한 방법은 위의 `<정밀도 팩터>`를 조정하는 것입니다: float32에서 4비트로 가면 메모리 요구 사항이 8배 줄어듭니다!
그러나 너무 낮은 정밀도를 사용하면 결과가 나빠질 수 있으므로, 일부 모델(특히 중간 범위)의 경우 float16 또는 8비트를 유지하는 것이 좋을 수 있습니다. (양자화는 매우 큰 모델의 성능에 덜 영향을 미치는 것 같습니다. 아마도 정보 중복성 때문일 것입니다.)
#### 모델 병렬 처리
모델 병렬 처리에는 모델을 더 작은 하위 모델 조각으로 잘라서 이러한 작은 조각들을 각각 다른 단일 GPU에 로드하고 실행하는 다양한 기술이 포함됩니다. 전체 모델을 한 번에 로드하지 않기 때문에 메모리가 적게 필요하지만 더 느릴 수 있습니다.

주요 모델 병렬 처리 유형은 2가지입니다
- 파이프라인 병렬 처리: 모델이 전체 레이어 수준에서 분할되고 레이어가 다른 GPU에 분배됩니다. 레이어 1의 출력이 레이어 2의 입력이므로, GPU가 대기하는 동안 유휴 상태가 되어 실행 속도가 느려집니다. 이를 "버블"이라고 합니다(그리고 데이터는 한 GPU에서 다음 GPU로 전송되어야 합니다). 입력을 더 작은 배치로 분할하면 버블을 줄일 수 있습니다. 이는 `PiPPy` [라이브러리](https://github.com/pytorch/PiPPy)와 함께 PyTorch에 기본적으로 추가되고 있으며, 이는 `accelerate`가 병렬 처리를 위해 내부적으로 사용하는 방식입니다.
- 텐서 병렬 처리: 모델이 행렬 계산 수준에서 분할됩니다. 이는 행렬이 행이나 열로 분할되고 전체 결과가 집계된다는 의미입니다. 모든 GPU가 동일한 노드에 있는 한(노드 간 네트워크 병목 현상을 피하기 위해) 이는 매우 효율적이지만 코딩하기 어려울 수 있습니다. `vllm` 라이브러리에서 이에 대한 멋진 구현을 찾을 수 있습니다. 이는 **놀라운 속도 향상**을 제공합니다.

다양한 종류의 병렬 처리(속도 향상을 위한 데이터 병렬 처리 포함)에 대한 가장 좋은 문서는 [여기](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism)에 있습니다.

#### CPU 오프로딩
CPU 오프로딩은 GPU 메모리 사용량을 줄이기 위해 일부 계산과 모델 부분을 CPU로 이동합니다. 데이터를 한 장치에서 다른 장치로 계속 이동해야 하기 때문에 여기에 있는 다른 방법보다 **상당히 느립니다**.

이에 대한 예시는 Deepspeed의 [ZeRO-Offload](https://arxiv.org/abs/2101.06840)로, CPU와 GPU 사이에 파라미터를 분산합니다(ZeRO-2 논문에 설명된 다른 최적화 위에). CPU에는 최적화 중에 그래디언트, 옵티마이저 상태 및 fp32 모델 파라미터 계산이 전달되는 반면, GPU에서는 CPU 메모리 사용과 GPU 계산을 활용하면서 둘 사이의 통신을 최소화하기 위해 fp16 파라미터와 순방향/역방향 패스를 찾을 수 있습니다.

### 내 모델이 GPU에 맞지만 여전히 OOM(Out of Memory) 오류가 발생합니다!
그렇다면 컨텍스트 크기에 문제가 있을 가능성이 높습니다.

다음을 권장합니다:
1) 로드된 일부 더미 추론 데이터로 모델이 실제로 GPU에 맞는지 테스트하십시오. 이 더미 추론 데이터는 충분히 큰 컨텍스트 크기(작업을 대표하는)를 가져야 합니다.
2) 배치 크기를 줄이거나, 활성화된 경우 우발적인 OOM 오류로 이어질 수 있는 자동 배치 크기 검색을 제거하십시오.
3) 더 일반적으로, 샘플이 역순 컨텍스트 크기 순으로 모델에 제시되도록 하여 컨텍스트 크기가 너무 크면 모델이 X시간 동안 실행된 후가 아니라 직접 실패하도록 하십시오.