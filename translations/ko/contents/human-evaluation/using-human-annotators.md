# 인간 평가자 활용하기

데이터 평가 품질에 관한 좋은 관행에 대한 이 [리뷰](https://aclanthology.org/2024.cl-3.1/)의 3장을 읽어보시길 권장합니다. 만약 프로덕션 수준의 품질을 원하고 이러한 방법들을 모두 구현할 수 있는 수단이 있다면, 시도해 보세요!

  ![최고의_평가_방법](https://github.com/huggingface/evaluation-guidebook/blob/main/assets/best_annotation_practices.png?raw=true)

그러나 작업과 평가 기준을 정의한 후에는 다음과 같은 중요한 가이드라인이 있습니다(프로젝트 규모와 상관없이).

- **인력 선정, 그리고 가능하다면 금전적 인센티브**
작업에 참여하는 사람들에게 다음을 원할 것입니다:
1) 특정 인구 통계학적 특성을 갖추도록 합니다.
	예를 들어: 대상 언어의 원어민, 높은 교육 수준을 가진 사람, 특정 도메인의 전문가, 지리적 출신 배경이 다양한 사람 등.
	필요한 요소는 작업에 따라 달라집니다.
1) 고품질 작업을 수행하도록 합니다.
	특히 요즘에는 답변이 LLM으로 생성된 것인지 확인하는 방법을 추가하는 것이 중요하며, 일부 평가자를 평가 풀에서 제외해야 할 수도 있습니다.
  *개인적인 의견으로는, 매우 의욕적인 크라우드소싱 평가자에 의존하는 경우가 아니라면 항상 평가자에게 제대로 보상하는 것이 좋습니다.*

- **가이드라인 설계**
가이드라인에 대해 정말 많은 시간을 들여 브레인스토밍하세요! 이는 [GAIA](https://huggingface.co/gaia-benchmark) 데이터셋에서 가장 많은 시간을 할애한 부분 중 하나입니다.

- **반복적 평가**
평가자들이 가이드라인을 오해할 수 있으므로(생각보다 더 모호할 수 있습니다) 여러 차례의 평가 라운드를 시도할 준비를 하세요! 샘플을 여러 번 생성하면 평가자들이 실제로 필요한 것에 수렴하는 데 도움이 됩니다.

  - **품질 평가** 및 **수동 큐레이션**
답변을 통제하고(가능하다면 평가자 간 일치도를 통해) 최종 선택을 통해 가장 높은 품질/가장 관련성 높은 답변만 유지하세요.

[Argilla](https://argilla.io/)와 같은 고품질 데이터셋을 구축하기 위한 특화된 도구도 도움이 될 수 있습니다.

### 더 알아보기
- ⭐ [몇 분 안에 자신만의 평가자 플랫폼을 설정하는 방법](https://huggingface.co/learn/cookbook/enterprise_cookbook_argilla), Moritz Laurer 작성. 오픈 소스 도구(Argilla 및 Hugging Face와 같은)를 사용한 실제 경험을 얻고, 대규모 인간 평가의 해야 할 것과 하지 말아야 할 것을 더 잘 이해하기 위한 좋은 읽을거리입니다.
- ⭐ [평가 모범 사례 가이드](https://aclanthology.org/2024.cl-3.1/). 2023년부터의 인간 평가에 관한 모든 논문을 검토한 것으로, 매우 포괄적입니다. 약간 밀도가 높지만 매우 이해하기 쉽습니다.
- [또 다른 평가 모범 사례 가이드](https://scale.com/guides/data-labeling-annotation-guide), 인간 평가에 특화된 ScaleAI의 자료. 위 문서의 더 가벼운 보완 자료입니다.
- [인간 라벨을 캡처하는 데 있어서의 가정과 도전](https://aclanthology.org/2024.naacl-long.126/)은 평가자 간 불일치의 원인과 실제 완화 방법에 관한 논문입니다.