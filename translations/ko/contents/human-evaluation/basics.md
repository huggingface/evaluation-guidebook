# 기본 개념

## 인간 평가란 무엇인가요?
인간 평가는 간단히 말해 사람들에게 모델을 평가하도록 요청하는 것입니다.
이 문서에서는 사후 평가에 초점을 맞춥니다: 모델 학습이 완료되었고, 특정 작업을 염두에 두고 있으며, 사람들이 점수를 제공하는 상황입니다.

### 체계적 평가
체계적인 방식으로 평가를 수행하는 주요 방법에는 3가지가 있습니다.

**데이터셋이 없는 경우**, 여러 능력을 탐색하고 싶다면, 사람들에게 작업과 채점 가이드라인(예: `두 모델에 유해한 언어를 출력하도록 시도해보세요; 유해한 언어를 출력한 모델은 0점, 그렇지 않으면 1점`)을 제공하고, 상호작용할 수 있는 하나(또는 여러) 모델에 접근 권한을 주면서 점수와 그 이유를 제공하도록 요청합니다.

**이미 데이터셋이 있는 경우**(예: `모델이 응답하지 않길 원하는 프롬프트 집합`), 해당 프롬프트로 모델에 질문하고, 프롬프트, 출력 및 채점 가이드라인을 사람들에게 제공합니다(`모델이 개인 정보를 답변하면 0점, 그렇지 않으면 1점`).

마지막으로, **데이터셋과 점수가 이미 있는 경우**, [오류 주석(error annotation)](https://ehudreiter.com/2022/06/01/error-annotations-to-evaluate/)을 통해 평가 방법을 검토하도록 사람들에게 요청할 수 있습니다(*이는 위 범주에서 채점 시스템으로도 사용될 수 있습니다*). 이는 새로운 평가 시스템을 테스트하는 중요한 단계이지만, 기술적으로는 평가에 대한 평가에 해당하므로 여기서는 약간 벗어난 범위입니다.

참고: 
- *이미 배포된 프로덕션 모델의 평가를 위해서는 사용자에게 피드백을 요청하고 A/B 테스트를 수행할 수도 있습니다.*
- [AI audits](https://arxiv.org/abs/2401.14462)(모델의 외부 체계적 평가)는 일반적으로 인간 기반이지만, 이 문서의 범위를 벗어납니다.

### 비공식 평가
인간 기반 평가를 수행하는 다른 두 가지 접근 방식이 있으며, 더 비공식적인 방식으로 진행됩니다.

**분위기 확인(Vibes-checks)**은 개인이 수행하는 수동 평가로, 일반적으로 비공개 프롬프트에 대해 다양한 사용 사례(코딩부터 작성된 성인물의 품질까지)에서 모델이 얼마나 잘 수행되는지 전반적인 느낌을 얻습니다. 주로 트위터와 레딧에서 공유되며, 결과는 대부분 일화적 증거를 구성하고 확증 편향에 매우 민감한 경향이 있습니다(다시 말해, 사람들은 찾고자 하는 것을 찾는 경향이 있습니다). 그러나 이는 [자신의 사용 사례를 위한 좋은 출발점](https://olshansky.substack.com/p/vibe-checks-are-all-you-need)이 될 수 있습니다.

**아레나(Arenas)**는 모델의 순위를 매기기 위한 크라우드소싱 인간 평가입니다.
이에 대한 잘 알려진 예는 [LMSYS 챗봇 아레나](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)로, 커뮤니티 사용자들은 두 모델 중 하나가 더 낫다고 판단할 때까지 모델과 채팅하도록 요청받습니다. 그런 다음 투표를 Elo 랭킹(경기 순위)으로 집계하여 어떤 모델이 "최고"인지 선택합니다.

## 인간 평가의 장단점

인간 평가는 다음과 같은 이유로 매우 흥미롭습니다:
- **유연성**: 평가하려는 것을 충분히 명확하게 정의하면, 거의 모든 것에 대한 점수를 얻을 수 있습니다!
- **오염 부재**: 시스템을 테스트하기 위한 새로운 질문을 사람들에게 작성하도록 요청하면, 이러한 질문은 학습 데이터에 존재하지 않아야 합니다(희망적으로).
- **인간 선호도와의 상관관계**: 평가에 인간 선호도를 사용하기 때문에 꽤 분명합니다.
  *참고: 그러나 인간으로 평가할 때는 결과가 일반화되도록 평가자의 다양성을 확보해야 합니다.*

그러나 몇 가지 제한 사항도 있습니다:
- **첫인상 편향**: 인간 평가자들은 실제 사실성이나 충실도 대신에 [첫인상에 기반하여](https://arxiv.org/abs/2309.16349) 답변의 품질을 평가하는 경향이 있습니다.
- **어조 편향**: 크라우드소싱 평가자들은 특히 어조에 매우 민감하며, 단정적인 답변에서의 사실적 또는 논리적 오류의 수를 과소평가합니다. 다시 말해, 모델이 자신감 있는 어조로 잘못된 것을 말하면, 인간 평가자들이 이를 알아차릴 가능성이 훨씬 적어져 더 단정적인 모델에 유리한 평가가 이루어질 수 있습니다. (전문가 평가자들은 이러한 편향에 덜 취약합니다.)
- **자기 선호 편향**: 인간은 [사실적으로 정확한 답변보다는 자신의 견해에 호소하거나 의견이나 오류와 일치하는 답변을 선호할 가능성이 높습니다](https://arxiv.org/abs/2310.13548).
- **정체성 편향**: 다른 정체성을 가진 사람들은 다른 가치관을 가지는 경향이 있어, 모델 답변을 매우 다르게 평가합니다(예: [유해성](https://arxiv.org/abs/2205.00501)에 대해).

### 체계적 인간 평가
특히 유료 평가자를 사용한 체계적 인간 평가의 장점은 다음과 같습니다:
- **사용 사례에 맞는 고품질 데이터 획득**으로, 나중에 활용할 수 있습니다(예: 선호도 모델을 개발해야 하는 경우).
- **데이터 개인정보 보호**: 특히 사내 유료 인간 평가자에 의존하는 경우, 데이터셋은 상대적으로 안전하게 유지됩니다. 반면 폐쇄형 API 모델을 사용한 LLM 평가는 외부 서비스에 데이터를 전송하므로 데이터가 어떻게 처리되는지에 대한 보장이 적습니다.
- **설명 가능성**: 모델이 얻은 점수는 평가한 인간에 의해 설명될 수 있습니다.

체계적 인간 평가에는 몇 가지 추가적인 문제가 있습니다:
- **비용**: 평가자에게 적절하게 보상을 지급하면 비용이 빠르게 증가할 수 있습니다. 또한 가이드라인을 개선하기 위한 여러 차례의 반복적 평가가 필요할 가능성이 높아 비용이 추가됩니다.
- **확장성 부족**: 사용자 피드백이 있는 프로덕션 시스템을 평가하는 경우가 아니라면, 인간 평가는 확장성이 낮습니다. 각 새로운 라운드마다 새로운 평가자를 동원(및 비용 지급)해야 하기 때문입니다.
- **재현성 부족**: 정확히 동일한 평가자를 계속해서 유지하고 가이드라인이 완벽하게 명확하지 않는 한, 일부 평가는 정확하게 재현하기 어려울 가능성이 높습니다.

### 비공식 인간 평가
비공식 인간 평가의 장점:
- **낮은 비용**: 군중의 선의에 의존하기 때문입니다.
- **예외 케이스 발견**: 대부분 제한이 없는 방식에서 사용자의 창의성을 활용하므로, 흥미로운 예외 사례를 발견할 수 있습니다.
- **더 나은 확장성**: 관심이 있고 기꺼이 참여하려는 많은 참가자가 있는 한, 비공식 인간 평가는 더 잘 확장되며 진입 비용이 낮습니다.

평가자 선택 없이 비공식적 접근 방식의 명백한 문제점:
- **높은 주관성**: 특히 평가자의 선호도가 [문화적으로 제한되는 경향](https://arxiv.org/abs/2404.16019v1)이 있기 때문에, 광범위한 가이드라인을 사용하는 많은 커뮤니티 구성원으로부터 일관된 평가를 강제하기 어렵습니다. 투표의 규모를 통해 "군중의 지혜" 효과(Galton의 위키피디아 페이지 참조)로 이러한 효과가 완화되기를 바랄 수 있습니다.
- **대표성 없는 선호도 순위**: 젊은 서구 남성이 인터넷의 기술 분야에서 과대 대표되므로, 이는 탐색된 주제와 전반적인 순위 모두에서 일반 대중의 선호도와 일치하지 않는 매우 편향된 선호도로 이어질 수 있습니다.
- **조작하기 쉬움**: 필터링되지 않은 크라우드소싱 평가자를 사용하는 경우, 제3자가 평가를 조작하기 쉽습니다. 예를 들어 특정 모델의 점수를 높이기 위한 경우(일부 모델은 독특한 글쓰기 스타일을 가지고 있기 때문)입니다.