# 基础概念

*注：本文内容与我写的 [通用评估博客](https://huggingface.co/blog/clefourrier/llm-evaluation) 存在部分重叠*
## 什么是自动评估基准？

自动化基准测试通常按照以下方式工作：你希望了解你的模型在某些方面的表现。这些“某些方面”可以是一个明确定义的具体任务，例如“我的模型在垃圾邮件分类中的表现如何？”，也可以是一个更抽象和通用的能力，例如“我的模型的数学能力有多强？”。

基于此，你可以通过以下方式构建评估：

数据集：
数据集由多个样本组成。这些样本包含模型的输入，有时还包括一个参考答案（称为“gold”），用于与模型的输出进行比较。
样本的设计通常是为了尽量模拟你想测试模型的场景。例如，如果你在研究电子邮件分类，你可以创建一个包含垃圾邮件和非垃圾邮件的样本数据集，并尝试加入一些具有挑战性的边界案例等。

评估指标：
评估指标用于对模型进行评分。例如：你的模型对垃圾邮件的分类准确度如何？正确分类的样本得分为1，错误分类的得分为0。
评估指标使用模型的输出来进行评分。在大型语言模型（LLMs）的情况下，人们主要关注两种输出：

模型根据输入生成的文本（生成式评估，generative evaluation）
提供给模型的一个或多个序列的对数概率（多项选择评估，有时称为 MCQA，或者困惑度评估 perplexity evaluations）
有关更多信息，请查看[模型推理与评估页面](https://huggingface.co/docs)。

在模型没有见过 (即未出现在训练集) 的数据上进行评估会更有意义，得出的模型 **泛化性** 结论才更准确。比如在只见过假冒银行垃圾邮件的模型上测试其能否正确分类与 “健康” 相关的垃圾邮件。

注：*模型只能在训练数据上预测效果良好 (没有隐式地学习到更高层次的通用范式) 的现象叫做 **过拟合**。这就类似于一个学生死记硬背了考试题目，却没有理解背后的知识点。所以只用训练集中的数据测试评估 LLM 得到的分数指标实际上是模型不具备的能力。*

## 自动评估基准的优劣势
优势：
- **一致性和可重复性**：在同一个模型上运行相同的自动评估基准 10 次，测试结果也是相同的 (除非受到硬件或模型自身随机性的影响)。所以相同任务下，多个模型的测试排名结果是公正的。
- **低成本规模效益**：目前自动评估基准是评估模型成本最低的方式之一。
- **易于理解**：大部分自动化方式的评价指标理解起来都非常容易。
  *例如：精确匹配可以理解为生成文本跟参考文本是否完全一致；准确率可以理解为做出的选项有多大程度是正确的 (不过对于像 `BLEU` 或 `ROUGE` 这种评价方式，理解难度会稍微高一些)。*
- **高质量测试集**：许多自动评估基准的测试集都来自专家级生成数据集或现有的高质量数据集 (如 MMLU 或 MATH)。当然也不是说这些测试集就完美无瑕，例如 MMLU 就被发现存在一些解析错误以及事实谬误，所以后来出现了一批改进的数据集，如 MMLU-Pro 和 MMLU-Redux。

劣势：
- **复杂任务难以保证效果**：自动评估基准通常在测试效果容易定义和评估的任务上表现良好 (如分类任务)。一旦任务比较复杂而且难以拆分为目标明确的子任务时，表现可能不及预期。
  *例如：测试模型的 “数学能力” 任务。具体是算术、还是逻辑、亦或是推演新数学概念的能力？*
  所以出现了一些无需拆分为子任务的 **通用性** 评估方式，由此评估出的模型整体表现就是评估目标的 **优良代理**。 
- **数据污染**：网络上的数据一旦以纯文本的形式公开，那么由于数据爬虫，这些数据总归会出现在模型训练集中。所以在评估时很难保证模型真的没有见过测试集。
