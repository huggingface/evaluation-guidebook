# 奖励模型相关内容

## 什么是奖励模型？

奖励模型通过学习人工标注的成对 prompt 数据来预测分数，优化目标是对齐人类偏好。
训练完成后，奖励模型可以作为人工评估代理的奖励函数，用来改进其他模型。

### 成对比较评分

最常见的奖励模型类型是 Bradley-Terry 模型，它的输出是一个分值，遵循以下公式：

$$p(\text{答案 b 优于答案 a}) = \text{sigmoid}(\text{score}_b - \text{score}_a)$$

奖励模型的训练数据只需要成对比较的答案，这比收集分数数据更容易。因此训练好的模型只能比较同一个 prompt 下的多个答案孰优孰劣，无法跨 prompt 比较。

其他模型在此方法的基础上进行了扩展，可以预测一个回答优于另一个的概率值 (例如 [基于 LLaMA3 的奖励模型](https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B))。

这样模型就能 (理论上) 以数值来判断多个回答之间的细微差别，不过只能针对同一 prompt 对应的回答进行对比，跨 prompt 的回答概率值就没有对比意义了。另外当回答较长时，可能会受到上下文长度和内存限制的影响。

### 绝对分数

还有一些奖励模型 (如 [SteerLM](https://arxiv.org/abs/2311.09528)) 的输出是绝对分数。这类模型使用起来更加方便，可以直接对回答评估分数，而无需构造成对。但是数据收集就比较困难了，因为在衡量人类偏好时，绝对分数就显得相对不那么稳定。

最近有人提出了更强力的模型，可以同时输出绝对分数和相对分数。如 [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257) 和 [ArmoRM](https://arxiv.org/abs/2406.12845)。


## 奖励模型用于评估的方法

给定一个 prompts 数据集，输入 LLM 生成回答，并请求奖励模型对回答评分。

如果使用的奖励模型输出是绝对分数，可以对所有回答的分数求平均来获取最终得分。

其实更常用的奖励模型输出是相对分数，对其求平均可能会受到异常值的影响 (某些非常好或非常差的回答)，因为不同 prompt 的评估分数可能具有不同的尺度 (某些 prompt 会比其他的更简单或困难)。

总上，我们可以使用： 
- 胜率 (win rate)：取一组参考回答，计算模型回答优于参考回答的百分比，这种结果会更加精细。
- 胜算概率 (win probabilities)：取一组参考回答，计算模型回答优于参考回答的平均概率，这种结果能够提供更细致和平滑的信号。

## 奖励模型的优劣势

优势：
- **非常迅速**：奖励模型只需要得到一个分数 (与 LLM 评估模型需要得到长文本不同)，因此其推理速度和小模型速度相当。
- **具有确定性**：前向过程相同，最终得分也会保持一致。
- **对位置偏差不敏感**：大多数奖励模型一次只处理一个回答，所以很少受到顺序的影响。即使对于需要处理成对回答的奖励模型，只要它在训练时使用的数据在顺序上是均衡的，受位置偏差的影响很非常小。
- **无需 prompt 工程**：很明显，奖励模型的任务目标只有一个，就是通过训练偏好数据来对一个或两个回答输出分数。

劣势：
- **需要特定微调**：即便微调后继承了基础模型的许多能力，不过在超出训练集范围的任务上可能还是表现不佳，另外这一步的成本会相对偏贵。
- **在强化学习和任务评估领域 (或使用直接对齐算法处理与奖励模型训练集相似的数据集时) 效率较低**：语言模型会过拟合奖励模型的偏好数据。

## 使用奖励模型进行评估的技巧与提示

- [RewardBench Leaderboard](https://huggingface.co/spaces/allenai/reward-bench)：奖励模型排行榜，可以找到很多高性能模型。
- [Nemotron](https://arxiv.org/abs/2406.11704) 论文中介绍了奖励模型的使用经验。
- 对于那些仅评分单个 prompt 与回答的奖励模型，可以缓存多个模型结果，当测试新模型的表现时就能够很快得到结论。
- [这篇论文](https://arxiv.org/abs/2410.11677v1) 对训练过程中的胜率或胜率概率进行了追踪整理，可以帮助检测模型退化以及选择最佳权重。
