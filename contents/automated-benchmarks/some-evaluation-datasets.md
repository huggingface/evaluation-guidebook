# Some evaluation datasets

If the task you are interested is already well studied, chances are that a dataset exists for it.

Below are a number of evaluation datasets which were developed in the last few years. However, careful:
- These are, for most of them, a bit "old", in the sense that they were designed pre-LLM. As such, they aimed to investigate one specific property of text (translation, summarization) which is no longer really how we evaluate models (evaluations are now more general/holistic) 
	  (*If you've got some bandwidth, this could really benefit from adding the publication dates!*)
	  (*This will also be updated with post LLM evals at some point*)
- They are likely contaminated, as they have been publicly on the web for a number of years. However, it doesn't mean they won't hold signal for your task!

## Pre-LLM datasets

| Evaluation name                                | Task type                                                                 | Task data                                                                                                                                                                                                                                                                                                    | Task content                                                                                                                                                                                                   | Source                                                                                      | Dataset                                                                                                                                                                                                                                                                                                                                    | Comments                                                                                                                                                                                                                                                  |
| ---------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| DeepFix                                        | Code task, Code-to-code, Correction                                       | 7K student-written erroneous C programs                                                                                                                                                                                                                                                                      | Correct the C programs                                                                                                                                                                                         | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/10742)                             |                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| MLSum                                          | Generation, Multilingual, Summarization                                   | 1.5M news summary/article pairs from the DailyMail, Le Monde, Süddeutsche Zeitung, El Pais, Moskovskij Komsomolets and Internet Haber (en, fr, de, es, ru, tur)                                                                                                                                              | Summarize the articles                                                                                                                                                                                         | [Paper](https://arxiv.org/abs/2004.14900)                                                   | [Hugging Face](https://huggingface.co/datasets/mlsum)                                                                                                                                                                                                                                                                                      | Palm: Prefixed with a prompt, truncated article to 2048 tokens                                                                                                                                                                                            |
| TransCoder                                     | Code task, Code-to-code                                                   | 852 parallel functions in Python/Java/C++                                                                                                                                                                                                                                                                    | Translate from a language to another                                                                                                                                                                           | [Paper](https://arxiv.org/pdf/2006.03511.pdf)                                               | [From paper](https://github.com/facebookresearch/CodeGen/blob/main/docs/transcoder.md)                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                           |
| WMT                                            | Multilingual, Translation                                                 | Datasets from the WMT conf on machine translation - datasets available depend on the year                                                                                                                                                                                                                    | Translate from a language to another                                                                                                                                                                           | [Conference](https://www.statmt.org/wmt20/) <br>Replace the 2 digits by the conference year |                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| Adversarial NLI                                | Language Inference                                                        | 10K entailment dataset generated using human in the loop adversarial attacks, looking for predicates which force models to predict wrong entailement labels (uses contexts from StoryCloze, CommonCrawl, Wikipedia, the Open Annotated National Corpus, WikiHow and GLUE)                                    | Predict entailment                                                                                                                                                                                             | [Paper](https://arxiv.org/abs/1910.14599)                                                   | [Data](https://dl.fbaipublicfiles.com/anli/anli_v1.0.zip  )<br>[Github](https://github.com/facebookresearch/anli)                                                                                                                                                                                                                          | R1 to R3 = rounds of data generation                                                                                                                                                                                                                      |
| APPS                                           | Text-to-code                                                              | 10K Python coding problems in natural languages, scraped from leetcode sites, with a suite of test cases.                                                                                                                                                                                                    | Solve the Python problem                                                                                                                                                                                       | [Paper](https://arxiv.org/abs/2105.09938)                                                   | [Github](https://github.com/hendrycks/apps) <br>[Data](https://people.eecs.berkeley.edu/~hendrycks/APPS.tar.gz)                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| AQuA                                           | Arithmetic, Reasoning                                                     | 100K multiple choice problems (GMAT, GRE, other sources) with question/options/rationale                                                                                                                                                                                                                     | Select the correct MCQA                                                                                                                                                                                        | [Paper](https://arxiv.org/abs/1705.04146)                                                   | [Github](https://github.com/deepmind/AQuA)                                                                                                                                                                                                                                                                                                 | Best results obtained with an external calculator added                                                                                                                                                                                                   |
| ARC                                            | Common Sense, Reasoning                                                   | 8K Grade school science questions: e = easy set, c = challenge set                                                                                                                                                                                                                                           | Select the correct MCQA                                                                                                                                                                                        | [Paper](https://arxiv.org/abs/1803.05457)                                                   | [Data](https://allenai.org/data/arc)                                                                                                                                                                                                                                                                                                       | Careful, this is the AI2 Reasoning Challenge, not the Abstraction and Reasoning Corpus                                                                                                                                                                    |
| ASDiv                                          | Analytical Reasoning, Arithmetic, Reasoning                               | 2.3K math world grade-school problems collected from various websites, with answers and level annotations by a Master’s student annotator (Could be high quality)                                                                                                                                            | Solve the problem                                                                                                                                                                                              | [Paper](https://aclanthology.org/2020.acl-main.92/)                                         | [Github](https://github.com/chaochun/nlu-asdiv-dataset)                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                           |
| bAbI                                           | Reasoning                                                                 | 20 tasks each with 2K automatically generated questions + short scenarios (successive actions generated with a simulated text adventure game).                                                                                                                                                               | Reason over the sentence to select the correct conclusion                                                                                                                                                      | [Paper](https://arxiv.org/abs/1502.05698)                                                   | [Github](https://github.com/facebookarchive/bAbI-tasks) <br>[Data](https://research.facebook.com/downloads/babi/)                                                                                                                                                                                                                          | See Part 4 for the simulation env and its constraints, it’s quite a fun idea. Probably not too hard to reproduce for other types of reasoning.                                                                                                            |
| BBQ                                            | Bias detection                                                            | 58K examples with two contexts (ambiguous and explicit about a bias), two questions (negative and non-negative) and possible answers, constructed from manual templates and checked with crowdsourcing.                                                                                                      | Predict the correct, non biased answer. Difference between accuracies depending on context/question allows to build a bias score.                                                                              | [Paper](https://aclanthology.org/2022.findings-acl.165/)                                    | [Github](https://github.com/nyu-mll/BBQ/tree/main/data)                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                           |
| BLiMP                                          | Language Understanding                                                    | 67 datasets of each artificially generated 1K minimal pairs testing syntactic, morphological and semantic knowledge, validated with MTurking.                                                                                                                                                                | Accuracy measured by looking if the log-probability the model assigned to the correct sentence is higher.                                                                                                      | [Paper](https://aclanthology.org/2020.tacl-1.25/)                                           | [Github](https://github.com/alexwarstadt/blimp/tree/master/data)                                                                                                                                                                                                                                                                           | Things tested: anaphor agreement, argument structure, binding, control/raisong, determiner-noun agreement, ellipsis, filler-gap, irregular forms, island effects, NPI licensing, quantifiers, subject-verb agreement                                      |
| BOLD                                           | Generation, Toxicity detection                                            | 23K prompts extracted from beginning of Wikipedia sentences containing a race/religion/political/gender/profession group member (ex: woman artist for gender=female). <br>                                                                                                                                   | Task: generating end of sentence, and toxicity is evaluated through a range of metrics (sentiment analysis, using classifiers, …).  In HELM, toxicity is measured using the Perspective API.<br>               | [Paper](https://arxiv.org/abs/2101.11718)                                                   | [Github](https://github.com/amazon-science/bold/tree/main/prompts)                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                           |
| BooksCorpus                                    | N/A                                                                       | 11K unpublished books of more than 20K words scraped from the web (of 16 different genres). <br><br>                                                                                                                                                                                                         | In the original paper, it's used to train a sentence embedding model - in model papers, it's often used for contamination or perplexity evaluations.                                                           | [Paper](https://arxiv.org/pdf/1506.06724.pdf)                                               | [Hugging Face](https://huggingface.co/datasets/bookcorpus)                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                           |
| BooksCorpus_HELM                               | Generation, Memorisation                                                  | 1K randomly sampled books from BooksCorpus.                                                                                                                                                                                                                                                                  | Task: from a random number of tokens beginning a paragraph, the model must generate a follow up - measure exact and near-exact reproduction.                                                                   | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Data](https://drive.google.com/file/d/10uC4jM6tgI1pgtq--07FFHQ2Te7-SXGA/view)                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                           |
| BoolQ                                          | Language Inference, Language Understanding                                | 16K sentences of naturally occurring yes no QA, from question + context from Wikipedia                                                                                                                                                                                                                       | Answer the MCQA                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1905.10044)                                                   | [Website](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| CB                                             | Language Understanding                                                    | 1.2K of discourse (news from the Wall Street Journal, fiction from the British National Corpus, dialogue from Switchboard), containing context + target sentence                                                                                                                                             | Predict commitment entailment                                                                                                                                                                                  | [Paper](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf)                         | [Website](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| Civil comments                                 | Toxicity detection                                                        | 1.8M online comments, with crowd sourced human labels for and toxicity following the Perspective API guidelines, and among these, 450K labeled with identity terms (crowdsourced, to pick in a list).                                                                                                        | Task : toxicity prediction, labels are used to identify areas of biases in models                                                                                                                              | [Paper](https://arxiv.org/pdf/1903.04561.pdf)                                               | [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)<br>[Hugging Face](https://huggingface.co/datasets/civil_comments)                                                                                                                                                                                     | Original paper contained a synthetic test set (77K examples generated from templates using 50 identity terms, 50/50 on toxicity) and a human labeled dataset (description in the Task content col) - I suppose the dataset available is the latter        |
| Clean E2E NLG                                  | Description, Generation                                                   | 50K crowdsourced generated descriptions of restaurants given keys and values (type of food = X, budget = Y, …).                                                                                                                                                                                              |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1706.09254)                                                   | [Hugging Face](https://huggingface.co/datasets/e2e_nlg_cleaned)                                                                                                                                                                                                                                                                            | Palm: Prefixed with a prompt, truncated article to 2048 tokens                                                                                                                                                                                            |
| CNN/DailyMail                                  | Cloze/Completion, Summarization                                           | Original dataset: 200K new documents (CNN/DailyMail between 2007 and 2015) converted into Cloze format by removing some of the text’s named entities, and using them as keys.  <br>                                                                                                                          | In HELM: Uses above documents (in complete form) as text to summarize, and their highlights as gold summaries.                                                                                                 | [Paper](https://arxiv.org/pdf/1506.03340.pdf)                                               | [Hugging Face](https://huggingface.co/datasets/cnn_dailymail) <br>[Data](https://cs.nyu.edu/~kcho/DMQA/)                                                                                                                                                                                                                                   | (I suspect this does not produce very good summaries though)                                                                                                                                                                                              |
| CommonsenseQA                                  | Common Sense, Reasoning                                                   | 12K turked Q/A (initialized from ConceptNet associations), then filtered by quality, with added context from a Google Search query > Some text likely overlaps with CC data                                                                                                                                  | Answer the MCQA                                                                                                                                                                                                | [Paper](https://aclanthology.org/N19-1421/)                                                 |                                                                                                                                                                                                                                                                                                                                            | Best results with an external calculator added                                                                                                                                                                                                            |
| Contrast Sets                                  | Generation, Robustness                                                    | 10 contrast sets of up to 1K examples, for their datasets (see comments) made by (often the original paper’s) researchers (increase reasoning steps in questions, replace words by their opposites, change counts …).                                                                                        | Follow original task setup with new examples, and see how/if performance drops.  <br>In HELM: use the IMDb and DROP contrast sets                                                                              | [Paper](https://aclanthology.org/2020.findings-emnlp.117/)                                  | [Data](https://allenai.org/data/contrast-sets)                                                                                                                                                                                                                                                                                             | NLVR2, IMDb sentiment analysis, MATRES Temporal RE, English UD parsing, PERSPECTRUM, DROP, Quoref, ROPES, BoolQ, MC-TACO.  <br>  <br>Dataset construction is way more detailed in Appendix                                                                |
| COPA                                           | Common Sense, Language Understanding                                      | 1K premise + causal questions with alternatives (common sense)                                                                                                                                                                                                                                               |                                                                                                                                                                                                                | [Paper](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)                 | [Website](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| CoQA                                           | In-Context Reading Comprehension                                          | 127K Conversational QA, from a given context (rationale must be provided too) - written by annotators                                                                                                                                                                                                        |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1808.07042)                                                   | v1.0 from [Data](https://stanfordnlp.github.io/coqa/)                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                           |
| DataImputation                                 | Real life task, Reasoning, Structured data                                | 8 structured datasets from several sources.                                                                                                                                                                                                                                                                  | Task: from a row of attributes with gaps, the model must fill the gaps (ex: extrapolating city from phone number, phone brand from its specs).  <br>                                                           | [Paper](https://sxsong.github.io/doc/21icde-imputation.pdf)                                 | [Data restaurant](https://www.cs.utexas.edu/users/ml/riddle/data/restaurant.tar.gz)<br>[Data Buy](https://dbs.uni-leipzig.de/file/Abt-Buy.zip)                                                                                                                                                                                             | See table 2 for all sources.<br>In HELM: use the subsets Buy and Restaurant, convert input to natural language, test accuracy.                                                                                                                            |
| Digits arithmetics (2D+, 2D-, 3D+, 3D-, …)     | Arithmetic                                                                | Basic arithmetic tasks for n digits addition, subtraction, composite operations with 2K example each                                                                                                                                                                                                        | Task: solve the math                                                                                                                                                                                           | [Paper](https://arxiv.org/pdf/2005.14165.pdf)                                               | [Github](https://raw.githubusercontent.com/openai/gpt-3/master/data/)                                                                                                                                                                                                                                                                      | All links come from the lm-evaluation-harness/lm_eval/datasets/arithmetic                                                                                                                                                                                 |
| DROP                                           | Arithmetic, In-Context Reading Comprehension                              | 55K adversarial questions which require 1) selecting relevant items from the text and 2) computing on them (sorting/counting/…) to get the correct answer                                                                                                                                                    | Task: select and count to provide the correct answer                                                                                                                                                           | [Paper](https://aclanthology.org/N19-1246/)                                                 | [Data](https://allenai.org/data/drop)                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                           |
| Dyck language_HELM                             | Symbolic manipulation                                                     | 500 D_n words between 52 and 100 characters (”word” made of nested brackets/parenthesis) where the last i characters have been removed.                                                                                                                                                                      | Task: Predict the unique sequence of closing parentheses.                                                                                                                                                      | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/dyck_language_scenario.py) <br>Also has a different version in BigBench                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| GSM8K                                          | Analytical Reasoning, Reasoning                                           | 8.5K diverse grade school math problems                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2110.14168v2)                                                 | [Github](https://github.com/openai/grade-school-math) <br>[Hugging Face](https://huggingface.co/datasets/gsm8k)                                                                                                                                                                                                                            | Best results with an external calculator added                                                                                                                                                                                                            |
| HellaSwag                                      | Cloze/Completion                                                          | 60K adversarially filtered multiple choice Q/A                                                                                                                                                                                                                                                               | Choose the correct next sentence (from captions or WikiHow)                                                                                                                                                    | [Paper](https://aclanthology.org/P19-1472/)                                                 | [Github](https://github.com/rowanz/hellaswag/tree/master/data)                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                           |
| HumanEval                                      | Code task, Text-to-code                                                   | 164 hand written programming problems with function signature, docstring, body + unit tests                                                                                                                                                                                                                  | Aim is to complete function to pass unit tests                                                                                                                                                                 | [Paper](https://arxiv.org/abs/2107.03374)                                                   | [Hugging Face](https://huggingface.co/datasets/openai_humaneval)                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| IMDB                                           | Sentiment Analysis                                                        | 50K reviews from IMDB, with even positive (score ≥ 7) /negative (score ≤ 4) reviews (no neutral).                                                                                                                                                                                                            | Classify positive/negative review.                                                                                                                                                                             | [Paper](https://aclanthology.org/P11-1015/)                                                 | [Website](https://ai.stanford.edu/~amaas/data/sentiment/)                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                           |
| LAMBADA                                        | Cloze/Completion                                                          | 10K Narrative contexts (from the BookCorpus) followed by a sentence where the last word is masked and must be predicted. Specifically built to force use of the context.                                                                                                                                     | Predict the last word.                                                                                                                                                                                         | [Paper](https://aclanthology.org/P16-1144/)                                                 | [Zenodo](https://zenodo.org/record/2630551#.YFJVaWT7S_w)                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                           |
| Language Modeling Evaluation_HELM              | Language Modeling                                                         | Compilation in HELM of several datasets: WikiText-103, ThePile (particularly arXiv, BooksCorpus2, Enron Emails, PubMed Central, Wikipedia), TwitterAAE, ICE.                                                                                                                                                 | Task: get conditional log probability of the full sequence (perplexity measure)                                                                                                                                | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [The pile website](https://pile.eleuther.ai/ )<br>[BLIMP Github](https://github.com/alexwarstadt/blimp )<br>[Wikitext data ](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip )<br>[Twitter AAE data](http://slanglab.cs.umass.edu/TwitterAAE/ )<br>[ICE data](https://www.ice-corpora.uzh.ch/en/access.htm) |                                                                                                                                                                                                                                                           |
| LegalSupport                                   | Entailment, Real life task, Reasoning                                     | 20K legal entailment scenarios, constructed from state/federal legal opinions (assertion is used as context, and 2 supporting sources (”see X, rule”) are selected at random).                                                                                                                               | Task: finding which rule best supports assertion.                                                                                                                                                              | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Data](https://docs.google.com/uc?export=download&id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77)                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                           |
| LinuxKernel_HELM                               | Generation, Memorisation                                                  | 2K randomly sampled functions from the Linux Kernel.                                                                                                                                                                                                                                                         | Task: from a random number of lines beginning a function, the model must generate a follow up - measure exact and near-exact reproduction.                                                                     | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Data](https://drive.google.com/file/d/1Y5piYwil7T6n8toT_-d7NWqVZHh9NVxJ/view)                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                           |
| LSAT                                           | Analytical Reasoning, In-Context Reading Comprehension, Logical Reasoning | 10K questions from the Law School Admission Test (analytical, logical reasoning, and reading comprehension), with context.                                                                                                                                                                                   | Answer the MCQA correctly                                                                                                                                                                                      | [Paper](https://arxiv.org/pdf/2108.00648.pdf)                                               | [Github](https://github.com/zhongwanjun/AR-LSAT/tree/main/data)                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| Magellan Benchmark                             | Real life task, Reasoning, Structured data                                | 23 datasets from several sources containing entities with attributes. Dirty datasets contain deliberate errors, such as attributes being in the wrong column, misspellings, etc.                                                                                                                          | Task: given two entities from two different tables, determine if they are the same or not.                                                                                                                     | [Paper](https://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf)                  | [Github](https://github.com/anhaidgroup/deepmatcher/blob/master/Datasets.md)                                                                                                                                                                                                                                                               | It’s likely that Abt-Buy and Buy are the same dataset                                                                                                                                                                                                     |
| MATH (Mathematics Aptitude Test of Heuristics) | Analytical Reasoning, Logical Reasoning, Reasoning, Symbolic manipulation | 12.5K mathematical problems from real competitions in natural language and latex.                                                                                                                                                                                                                            |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2103.03874)                                                   | [Data](https://people.eecs.berkeley.edu/~hendrycks/MATH.tar)                                                                                                                                                                                                                                                                               | HELM: uses a “use_chain_of_thought” flag                                                                                                                                                                                                                  |
| MAWPS                                          | Arithmetic, Reasoning                                                     | 3.3 K math world problems (problem + answer + template, from existing datasets: AddSub, SingleOp, MultiArith, SingleEq, SimulEq-S and SimulEq-L)                                                                                                                                                             | Solve the math problem                                                                                                                                                                                         | [Paper](https://aclanthology.org/N16-1136/)                                                 | [Github](https://github.com/sroy9/mawps)                                                                                                                                                                                                                                                                                                   | Best results with an external calculator added                                                                                                                                                                                                            |
| MBPP                                           | Code task, Text-to-code                                                   | 1K entry-level Python crowd-sourced programming problems (description, solution, 3 unit test cases) - (58% mathematical, 43% list processing, 19% string processing, 9% integer sequences, and 2% other).                                                                                                    | Solve the Python program                                                                                                                                                                                       | [Paper](https://arxiv.org/abs/2108.07732)                                                   | [Github](https://github.com/google-research/google-research/tree/master/mbpp )<br>[Hugging Face](https://huggingface.co/datasets/mbpp)                                                                                                                                                                                                     | Also contains an edited version (400 items) with unambiguous prompts and good signatures (can be interesting to look at later to see the impact of prompts on code gen) + a MathQA-Python dataset (adaptation of the MathQA dataset)                      |
| MMLU                                           | Language Understanding                                                    | 15K multi-choice Q/A manually collected from various online sources, on many topics (legal, philosophy, economics, psychology, STEM, medicine, etc, - at high school to professional level)                                                                                                                  | Answer the MCQA                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2009.03300)                                                   | [Hugging Face](https://huggingface.co/datasets/lukaemon/mmlu )<br>[Github](https://github.com/hendrycks/test )                                                                                                                                                                                                                             | Seems like a strong/high-quality baseline                                                                                                                                                                                                                 |
| MRF (Misinfo Reaction Frames)                  | Generation, Misinformation capabilities                                   | 200K pairs of claims from news headlines (climate change, covid 19, cancer illness, detailed sources in comments) + label (real/misinformation), the former annotated on veracity, likelihood of spread, writer intent by MTurk workers.                                                                     | Task: must either predict the gold label or generate likely writer intent/reader perception/…                                                                                                                  | [Paper](https://aclanthology.org/2022.acl-long.222/)                                        | [Github](https://github.com/skgabriel/mrf-modeling)                                                                                                                                                                                                                                                                                        | (Contains data from NELA-GT-2018-2020, SciDCC, Climate-FEVER, CoAID, CoronaVirusFacts/DatosCoronaVirusAlliance Database, ESOC Covid-19 Misinformation Dataset, DETERRENT)                                                                                 |
| MS MARCO                                       | Question Answering, Retrieval                                             | 1M anonymised questions with free-form human generated answers (from relevant web document extracts), some with added rewriting. <br>                                                                                                                                                                        | Original paper contains 3 tasks: 1) generating the correct answer, if possible, 2) same but answer should make sense even without context, 3) ranking 1000 passages on how relevant they are for the question. | [Paper](https://arxiv.org/abs/1611.09268)                                                   | [Github](https://microsoft.github.io/msmarco/)                                                                                                                                                                                                                                                                                             | Contains extended descriptions of QA datasets in lit review.<br>In HELM, only the ranking task is looked at, and relevance is estimated looking at the log-likelihood of the prediction when asking “Does the passage answer the query?”                  |
| MS MARCO TREC, aka TREC 2019                   | Retrieval                                                                 | Datasets derived from MS MARCO, edited for either passage or document retrieval tasks, either doing full retrieval or top-n reranking (100 for documents, 1000 for passages). (see MS MARCO)                                                                                                                 |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2003.07820)                                                   | [Data](https://trec.nist.gov/data/deep2019.html) <br>[Github](https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019.html)                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| MultiRC                                        | Language Understanding, Question Answering                                | 6K multiple choice question over a diversity of topics                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/N18-1023.pdf)                                              | [Data](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| NarrativeQA                                    | Question Answering, Retrieval                                             | 47K free-form human generated questions and answers, linked to 1.5K books (Gutemberg project) and movie scripts (scraped) matched with plot summaries. <br>                                                                                                                                                  | Task: from the summary or the story, answer or select the correct answer.                                                                                                                                      | [Paper](https://arxiv.org/abs/1712.07040)                                                   | [Github](https://github.com/deepmind/narrativeqa)                                                                                                                                                                                                                                                                                          | For long range context testing, we could use this dataset to do QA from the full stories. Could be interesting for anything conversational imo.                                                                                                           |
| Natural Questions                              | Open domain/Closed book, Question Answering                               | 207K aggregated google search queries + annotated wikipedia sample answer                                                                                                                                                                                                                                    |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/Q19-1026/)                                                 | [Data](https://ai.google.com/research/NaturalQuestions/download)                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| NewsQA                                         | Question Answering                                                        | 100K human generated QA pairs from 12K news articles (CNN). Questions were generated from title + summary, answers from question + article, then kept through a validation mechanism.  <br>Likely intersects with CNN/DailyMail, as data extraction script was the same.                                     |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/W17-2623/)                                                 | [Github](https://github.com/Maluuba/newsqa)                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                           |
| OpenBookQA                                     | Common Sense, Reasoning                                                   | 6K sentences, science reasoning needing common sense knowledge to extrapolate to new situations                                                                                                                                                                                                              |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1809.02789)                                                   | [Data](https://allenai.org/data/open-book-qa)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| PIQA                                           | Common Sense, Reasoning                                                   | 20K physical common sense reasoning situations,                                                                                                                                                                                                                                                              | select the correct action to do from a context and answers                                                                                                                                                     | [Paper](https://arxiv.org/abs/1911.11641)                                                   | [Data](https://yonatanbisk.com/piqa/data/)                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                           |
| PopularBooksCorpus_HELM                        | Generation, Memorisation                                                  | 20 books from BooksCorpus which appear in a list of bestsellers.                                                                                                                                                                                                                                             | Task: from a random number of tokens beginning the first paragraph of the book, the model must generate a follow up - measure exact and near-exact reproduction.                                               | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Data](https://drive.google.com/file/d/1RT29rRKNNXKgZBhXNbqevLwR440g44it/view)                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                           |
| QuAC                                           | In-Context Reading Comprehension                                          | 100K questions in information seeking QA contexts (used Wikipedia to generate dataset)                                                                                                                                                                                                                       |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/D18-1241/)                                                 | [Data](https://quac.ai/)                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                           |
| RACE                                           | In-Context Reading Comprehension                                          | 100K questions from English reading comprehension exam for Chinese mid/high school students                                                                                                                                                                                                                  |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/D17-1082/)                                                 | [Data](https://www.cs.cmu.edu/~glai1/data/race/)                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| RAFT                                           | Real life task, Text classification                                       | Compilation of 11 datasets of naturally occurring classification tasks, of between 150 and 5K test items.                                                                                                                                                                                                    | Task: in few shot from 50 labeled examples, provide meaningful labels. (Domains: medical, finance, research, english language, law, physics, AI safety, social networks)                                       | [Paper](https://arxiv.org/abs/2109.14076)                                                   | [Hugging Face](https://huggingface.co/datasets/ought/raft)                                                                                                                                                                                                                                                                                 | Corpus: (ADE Corpus v2, Banking77, NeurIPS 2020 impact statement risks, OneStopEnglish, Overrruling, Systematic review inclusion, TAI safety research, Terms of Service, TweetEval Hate, Twitter complaints, + Semiconductor org types, created for this) |
| RealToxicityPrompts                            | Generation, Toxicity detection                                            | 100K natural occurring sentences (selected from OpenWebText corpus, basically = reddit, and scored for toxicity with the PerspectiveAPI) split in two to create a prompt and continuation.                                                                                                                   | Task: generate the continuation from the sentence start, then toxicity evaluated with PerspectiveAPI.                                                                                                          | [Paper](https://arxiv.org/abs/2009.11462)                                                   | [Data](https://allenai.org/data/real-toxicity-prompts)<br>[Github](https://github.com/allenai/real-toxicity-prompts) (the repo lacks a lot of info)                                                                                                                                                                                        |                                                                                                                                                                                                                                                           |
| ReCoRD                                         | Language Understanding                                                    | 120K passage/cloze query/answer examples from news (CNN, DailyMail) with human filtering                                                                                                                                                                                                                     |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1810.12885)                                                   | [Data](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| RTE                                            | Language Understanding                                                    | 3K compilation of competition data on entailement                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                | [Paper](https://w4ngatang.github.io/static/papers/superglue.pdf)                            | [Data](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| SAT analogies                                  | Language Understanding                                                    | 374 SAT analogy problem prior to 2005 (a is to b what c is to multiple choice questions; words are not the most frequent)                                                                                                                                                                                    |                                                                                                                                                                                                                | [Paper](https://arxiv.org/pdf/2005.14165.pdf)                                               | [Data dev](https://goo.gl/XWjas1)  <br>[Data test](https://goo.gl/BcTtB4)                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                           |
| SIQA                                           | Question Answering                                                        |                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                |                                                                                             |                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| SQuADv2                                        | In-Context Reading Comprehension                                          | Combines SQuAD with 50K unanswerable questions                                                                                                                                                                                                                                                               | from a context, give an answer, but only if possible                                                                                                                                                           | [Paper](https://arxiv.org/abs/1806.03822)                                                   | [Github](https://rajpurkar.github.io/SQuAD-explorer/)                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                           |
| StoryCloze                                     | Cloze/Completion, Common Sense                                            | 50K 5-sentences commonsense stories                                                                                                                                                                                                                                                                          | choose the correct ending                                                                                                                                                                                      | [Paper](https://aclanthology.org/N16-1098/)                                                 | [Hugging Face](https://huggingface.co/datasets/story_cloze)                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                           |
| StrategyQA                                     | Common Sense, Reasoning                                                   | 2.8K questions needing reasoning from implicit knowledge                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2101.02235)                                                   |                                                                                                                                                                                                                                                                                                                                            | Best results with an external calculator added                                                                                                                                                                                                            |
| SVAMP                                          | Arithmetic, Reasoning                                                     | 1K math world problems (sampled from ASDivA, higher quality than MAWPS) with variations in framing                                                                                                                                                                                                           |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/2021.naacl-main.168/)                                      | [Github](https://github.com/arkilpatel/SVAMP/blob/main/SVAMP.json)                                                                                                                                                                                                                                                                         | Best results with an external calculator added                                                                                                                                                                                                            |
| Synthetic reasoning (natural)                  | Logical Reasoning, Reasoning                                              | Synthetic data generated on the fly, containing a set of synthetic rules (conditional statements), facts (attributes), and the logical gold output.                                                                                                                                                          |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2211.09110)                                                   | Can be generated with [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_natural_scenario.py)                                                                                                                                                                                       | Also called rule_induct in HELM                                                                                                                                                                                                                           |
| Synthetic reasoning (symbolic)_HELM            | Logical Reasoning, Symbolic manipulation                                  | Synthetic data generated on the fly using a pattern template.                                                                                                                                                                                                                                                | Either test if the model is able to identify patterns (”beach + beach - pear” has “A + A - B” as pattern) or if the model can substitute strings in a given pattern.                                           | [Paper](https://arxiv.org/abs/2211.09110)                                                   | Can be generated with [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_scenario.py)                                                                                                                                                                                               |                                                                                                                                                                                                                                                           |
| TriviaQA                                       | Open domain/Closed book, Question Answering                               | 95K trivia QA (compositional questions, syntactic variability)                                                                                                                                                                                                                                               |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/P17-1147/)                                                 | [Data](https://nlp.cs.washington.edu/triviaqa/)                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| TruthfulQA                                     | Question Answering                                                        | 817 questions about tricky factual claims (common misconceptions, falsehoods, …) over 38 categories, with true and false reference answers + a source to support true answers (+ 380 added questions).                                                                                                       |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/2109.07958)                                                   | [Github](https://github.com/sylinrl/TruthfulQA)                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| TyDiQA-GoldP                                   | Multilingual, Question Answering                                          | 204K multilingual QA pairs (unconstrained question elicitation from prompts, then Wikipedia article retrieval, and specific answer selection in the article if possible) (en, ar, ben, fin, ind, ja, ko, ru, tel, th and kiswahili).                                                                         | MCQA                                                                                                                                                                                                           | [Paper](https://aclanthology.org/2020.tacl-1.30/)                                           | [Github](https://github.com/google-research-datasets/tydiqa)                                                                                                                                                                                                                                                                               | Dataset generated can present interesting underspecification of questions and mismatch between question and answers language level. Might be harder than other datasets                                                                                   |
| Web Questions                                  | Open domain/Closed book, Question Answering                               | Extracted 100K “Wh?” questions from Google Search API, then annotated by MTurkers - I suspect answers are partially out of date                                                                                                                                                                              | MCQA                                                                                                                                                                                                           | [Paper](https://aclanthology.org/D13-1160/)                                                 | [Website](https://nlp.stanford.edu/software/sempre/)                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                           |
| WebNLG                                         | Generation, Verbalization                                                 | 13K mappings between triples (subject, property, object, constructed from DBPedia, which is a KB from Wikipedia) and sentence verbalization (by crowd workers), about specific topics (astronauts, universities, monuments, buildings, characters from comics, food, airports, sports teams, written works). | Task: verbalize in a grammatical way.                                                                                                                                                                          | [Paper](https://aclanthology.org/P17-1017.pdf)                                              | [Hugging Face](https://huggingface.co/datasets/web_nlg)                                                                                                                                                                                                                                                                                    | <br>There was a sentence selection for fluency and the sentences generated are relatively simple, but there is no description of annotators/crowdsourcers origins > maybe some data is not in “standard English”.                                         |
| WiC                                            | Language Understanding                                                    | 7K, classification of whether a word occurring in two different contexts has the same meaning or not                                                                                                                                                                                                         |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/N19-1128/)                                                 | [Site](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                           |
| WikiFact_HELM                                  | Cloze/Completion                                                          | 12 domains with 1K triples (subject, relation, object) sampled from Wikipedia and cleaned.                                                                                                                                                                                                                   | Task: predict missing item in the sentence made of the relation.                                                                                                                                               | [Paper](https://arxiv.org/abs/2211.09110)                                                   | [Codalab](https://worksheets.codalab.org/rest/bundles/0x8c3b60eb7c6b462e822a150f194d3b35/)<br>[Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/wikifact_scenario.py)                                                                                                                                  |                                                                                                                                                                                                                                                           |
| WikiLingua                                     | Generation, Multilingual, Summarization                                   | 43K article/summary pairs constructed from WikiHow in 18 languages (on the site, articles are written with a summary sentence + detailed paragraph per step: in the dataset, summaries are the concatenation of the summary sentences, and articles of the detailed paragraphs                               | Summarization                                                                                                                                                                                                  | [Paper](https://aclanthology.org/2020.findings-emnlp.360/)                                  | [Github](https://github.com/esdurmus/Wikilingua )                                                                                                                                                                                                                                                                                          | Palm: Prefixed with a prompt, truncated article to 2048 tokens<br>I suspect data creation can leads to very “robotic” language for the summary baseline, which could underscore more fluid summaries - though ROUGE shouldn’t be too prone to that).      |
| Winogender                                     | Bias detection                                                            |                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                |                                                                                             |                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                           |
| Winograd                                       | Reasoning, Winograd                                                       | 273 to 285 examples where one must disambiguate who/what a pronoun is referring to on sentence specially constructed to be ambiguous to statistics not to humans                                                                                                                                             | Disambiguation of pronoun                                                                                                                                                                                      | [Paper](https://dl.acm.org/doi/10.5555/3031843.3031909)                                     | [Website](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WSCollection.xml)                                                                                                                                                                                                                                                              | Not sure if GPT3 was evaled on this one or the SuperGLUE one                                                                                                                                                                                              |
| WinoGrande                                     | Reasoning, Winograd                                                       | 43K sentences Adversarial Winograd                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                | [Paper](https://arxiv.org/abs/1907.10641)                                                   | [Website](https://winogrande.allenai.org/)                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                           |
| WSC                                            | Language Understanding, Winograd                                          | WinoGrad Schema Challenge (see Winograd)                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                | [Paper](https://w4ngatang.github.io/static/papers/superglue.pdf)                            | [Website](https://super.gluebenchmark.com/tasks)                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                           |
| XSUM                                           | Summarization                                                             | 226K news articles (BBC, 2010 to 2017) matched with their single sentence summary (comes from the article). Task: Summarize. (Domains: News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts)                                                    |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/D18-1206/)                                                 | [Github](https://github.com/EdinburghNLP/XSum)                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                           |
| XSum                                           | Generation, Summarization                                                 | 226K news summary/article pairs from the BBC (2010 - 2017) extracted from the WayBack machine                                                                                                                                                                                                                |                                                                                                                                                                                                                | [Paper](https://aclanthology.org/D18-1206/)                                                 | [Hugging Face](https://huggingface.co/datasets/xsum)                                                                                                                                                                                                                                                                                       | Could be interesting to manually check if the model recent knowledge creates discrepancies in the summaries of old news.                                                                                                                                  |
## Dataset ideas to manually reproduce

| Evaluation name                | Task type                                                  | Task content                                                                                                                                                                                                                                                              | Source                                                                                     | Dataset                                                                                  | Comments                                                                                                                                                                                         |     |
| ------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --- |
| ✍️ GSM8K-Python                | Code task, Text-to-code                                    | Python version of the GSM8K dataset (8.5K grade school math problems)                                                                                                                                                                                                     | [Paper](https://arxiv.org/abs/2204.02311)                                                  | N/A                                                                                      |                                                                                                                                                                                                  |     |
| ✍️ MRF                         | Generation, Manual evaluation, Misinformation capabilities | 250 headlines extracted from the MRF dataset, grouped in 80 clusters by thesis. Task: from the thesis + 5 headlines, the model must generate plausible headlines which supports the thesis. Annotators evaluate if 1) the headline supports the thesis and 2) looks real. | [Paper](https://arxiv.org/abs/2211.09110)                                                  | [Data](https://drive.google.com/uc?export=download&id=1uVJbsgPCHFAvH43I6SVvU3Ayo8dh-y_N) | See [report](https://cset.georgetown.edu/publication/truth-lies-and-automation/) page 6 for a detailed explanation of the original process, plus sections 8.5.2, E.5, and 5.5 in the HELM paper. |     |
| ✍️ News article generation     | Generation                                                 | Generated 25 articles from titles and subtitles, 80 humans had to classify if generated or original                                                                                                                                                                       | [Paper](https://arxiv.org/pdf/2005.14165.pdf)                                              |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️  Numeracy Prediction        | Symbolic manipulation                                      | “requires the model to perform symbolic regression given a few examples, and apply the number relationship to a new input”                                                                                                                                                |                                                                                            | [Paper](https://arxiv.org/abs/2211.09110)                                                | [Github](https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/numeracy_scenario.py)                                                                                      |     |
| ✍️ SVG datasets                |                                                            | Could construct an SVG dataset to see if models can indeed generate or interpret SVG drawings                                                                                                                                                                            | [Twitter thread](https://twitter.com/zswitten/status/1631178997508997120)                  |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️ Theory of the mind datasets |                                                            | Could likely be easy to generate                                                                                                                                                                                                                                          | [Paper](https://arxiv.org/abs/2302.08399)                                                  |                                                                                          |                                                                                                                                                                                                  |     |
| ✍️ Wedging prompts             | Generation, Manual evaluation, Misinformation capabilities | 11 prompts with specific intent (ex: influence voting behaviors, target specific groups by generate pro/anti X rhetoric) augmented with 3 examples. Task: generate follow up examples.  <br>                                                                              | [Paper](https://cset.georgetown.edu/wp-content/uploads/CSET-Truth-Lies-and-Automation.pdf) | [Data](https://drive.google.com/uc?export=download&id=1kWB3_F4Tobc_oVGC_T-a5DHEh-AB4GTc) | In HELM: use manual evaluation to determine if the message generate 1) addresses targeted group; 2) supports desired message; 3) is divisive                                                     |     |
| ✍️ Word scrambling             | Symbolic manipulation                                      | 10K examples for 5 tasks of 5 character manipulation tasks (word with cycled letters, anagrammed, random insertions, reversed). Model needs to recover the original word                                                                                                  | [Paper](https://arxiv.org/pdf/2005.14165.pdf)                                              |                                                                                          | Easy to generate/automate, see Section 3.9.2 of GPT3 paper                                                                                                                                       |     |
